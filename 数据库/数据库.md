# 数据库

## 事务的四大特性

### 原子性

> 原子性是指事务包含的所有操作**要么全部成功，要么全部失败回滚，**

### 一致性

> 一致性是指事务必须从一**个一致状态变成另一个一致状态**，也就输说一个事务执行之前和执行之后，都必须处于一致状态

就拿转账来说，假设用户A和用户B两者加起来有5000，那么不管A和B之间如何转账，事务结束后，加起来还是5000，这就是一致性

### 隔离性

> 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，**不能被其他事务所干扰**，多个并发事务之间要相互隔离，

既要达到这么一种效果，对于两个并发的事务T1和T2，在事务T1看来，T2要么在T1开始前就已经结束才开始，要么就在结束之后才开始，这样每个事务都感觉不到其他事务在并发的执行，关于事务的隔离性提供多个隔离的级别

### 持久性

> 持久性是**指一个事务一旦被提交**，**那么对于数据库的数据改变就是永久的**，即使在数据库遇到故障的时候也不会丢失提交事务的操作

例如我们在使用JDBC操作数据库时，在提交事务方法后，提示用户事务操作完成，当我们的程序完成看到提示后，就可以认定事务以及正确提交，即使这时候数据库出现了问题，也必须要将我们的事务完全执行完成，否则就会造成我们看到提示事务处理完毕，但数据库因为故障没有执行事务的重大错误

## 事务常见问题

### 更新丢失

> 原因：**当多个事务选择同一行操作，并且都是基于最初选定的值，由于每个事务都不知道其他事务的存在，就会发生更新覆盖的问题。类比github提交冲突。**

### 脏读

>脏读是指在一个事务处理过程中，读取了另一个未提交的事务中的数据，**事务A读取了事务B已经修改但尚未提交的数据。若事务B回滚数据，事务A的数据存在不一致性的问题**（一致性问题）

当一个事务正在多次修改某个数据，而在这个事务中**多次修改都还未提交**，这时一个并发的事务来访问数据，就会造成两个数据不一致

例如：用户A向用户B转账100元，对应SQL命令如下

```sql
    update account set money=money+100 where name=’B’;  (此时A通知B)

    update account set money=money - 100 where name=’A’;
```

当只执行第一条SQL时，A通知B查看账户，**B发现确实钱已到账（此时即发生了脏读）**，而之后无论第二条SQL是否执行，只要该事务不提交，则所有操作都将回滚，那么当B以后再次查看账户时就会发现钱其实并没有转。

### 不可重复读

> 不可重复读是指对于数据库中的某项数据，一个事务范围**多次查询却返回了不同的数据值**。这是由于在**查询间隔，被另一个事务修改提交了**，**事务A第一次读取最初数据，第二次读取事务B已经提交的修改或删除数据。导致两次读取数据不一致。**（不符合事务的隔离性）

例如事务T1在读取某一个数据，而事务T2立马修改了这个数据并提交事务给数据库，事务T1再次读取该数据就得到了不同的结果，发生了不可重复读

不可重复读和脏读的区别是：**脏读是某一项事务读取了另一项事务未提交的脏数据，而不可重复读则是读取了前一个事务提交的数据**

在某些情况下，不可重复读并不是问题，比如我们多次查询某个数据当然以最后查询的结果为主，但在另一个情况下就可能发生问题，例如对于同一个数据A和B依次查询就可能不同，A和B就可能冲突

### 虚读（幻读）

> 幻读是事务**非独立执行时**发生的一种现象，**事务A根据相同条件第二次查询到事务B提交的新增数据，两次数据结果集不一致**。（不符合事务的隔离性）

例如一个事务T1对一个表中的所有行的某个数据项修改为1，而这是事务T2又对这个表中插入了一行数据，而该项数据数值可能还是为2，而操作事务T1的用户如果在查看刚刚修改的数据，会发现还有一行没有修改，其实这行是从T2中添加的，就像产生幻觉一样

## 事务有哪些隔离级别？

### 读未提交（Read Uncommitted）RU级别

> 读未提交，**顾名思义，就是可以读到未提交的内容，**
>
> 因此，在这种隔离级别下，查询是不会加锁的，也有由于查询不加锁，所以这种隔离级别的一致性是最差的，可能会产生脏读，不可重复读，幻读

### 读提交（Read Committed）RC级别

> 读提交，顾名思义，就是只能读到已经提交的内容
>
> 这就是各种系统中最常用的一种隔离级别，也就是也是SQL Server和Oracle的默认隔离级别
>
> 这种隔离级别能够有效避免脏读，但除非在查询中显示的加锁，不然普通查询是不会加锁的
>
> 但是为什么读提交和读未提交都**没有查询加锁**，但是能够有效避免脏读
>
> **这就要说到另一种机制：快照**
>
> **这种既能保证一致性又不加锁的读，也叫快照读**

假如没有快照读，那么当一个更新的事务没有提交时，另一个对更新数据进行查询的事务会因为无法查询而阻塞，这种情况下，并发能力就相当差

而“快照读”就可以完成高并发的查询，不过，“读提交”只能避免“脏读”，并不能避免“不可重复读”和“幻读”

### 可重复读(Repeated Read) RR级别

>可重复读，顾名思义，就是专门针对不可重复读，这种情况制定的隔离级别，自然，他就可以有效避免不可重复读，而它也是MySql的默认隔离级别。
>
>在这个级别下，**普通查询同样使用的是快照读，但是和读提交不同的是，当事务启动时，就不允许进行修改了，**而不可重复读恰恰是因为两次读取之间进行了数据的修改，因此，可重复读能够有效避免不可重复读，但却避免不了幻读，因为幻读是由于插入和删除操作而产生的

### 串行化（Serializable）SE级别

>这是数据库最高的隔离级别，这种级别下，事务“串行化顺序执行”，**也就是一个一个排队执行。**
>
>这种级别下，“脏读”、“不可重复读”、“幻读”都可以被避免，但是执行效率奇差，性能开销也最大，所以基本没人会用

### 小结

> 读未提交：什么都不能避免
>
> 读提交：能避免脏读，因为select没有规矩导致脏读
>
> 可重复读：能避免脏读，不可重复读，因为update没有规矩导致脏读，但是也只保证update
>
> 串行化：能避免脏读，不可重复读，幻读，但是失去了并发性，效率低



## 事务隔离级别的实现方案：LBCC&&MVCC

我们直到，在数据库中，事务并发操作会出现问题，那就是脏读，不可重复读，幻读

为了解决这三个问题，数据库有了四个隔离级别，

那么数据库是如何实现这写隔离级别的呢

总体来说，我们有两类方案：LBCC（Lock Based Concurrency Control）和MVCC（Multi-Version Concurrenct Control）

### LBCC 

第一种，既然我们要保证读取前后数据一致，那么我们在读的时候加锁，锁定我们要操作的数据，不允许其他的事务修改，那么不就可以了吗，这是第一种方案：叫基于锁的并发控制 Lock Based Concurrency Control）LBCC

如果仅仅是基于锁来实现并发控制，那么意味着不支持并发的读写操作，而大多数的应用都是读少写多，这种方式极大的影响了数据库的效率

并且我们锁定一条数据，并不能解决幻读问题

### MVCC

所以我们还有一种解决方案，如何要让一个事务读取数据前后一致，我们只需要在读取数据前给他建立一个备份，后面直接读取这个备份就行了，这种方案叫做多版本并发控制

MVCC的核心思想就是：我们可以查到我们这个事务开始之前的数据，而这个事务修改update或者被删除delete对我们来说是不可见的



## 数据库的三范式是什么？

### 什么是范式

* **当一个关系的所有分类都是不可再分的数据项的时候，该关系是规范化的**
  * 不可再分的数据项：即不**存在组合数据项和多项数据项**
* 一个低一级的数据项，通过模式分解可以转换为若干高一级范式的关系模式的集合，这个过程就叫规范化

### 第一范式条件

> **必须不包含重复组的关系，即每一列都是不可拆分的原子项**

![img](https://img-blog.csdnimg.cn/20190226224717807.png)

非规范化转换为规范化的第一范式很简单，讲标分别从横向和纵向展开即可，将高级职称横向展开即可以得到满足第一范式的表结构

![img](https://img-blog.csdnimg.cn/20190226224746882.png)

### 第二范式条件

> 关系必须满足第一范式，**并且所有非主属性都完全依赖于主码**，注意：符合第二范式的关系模型可能存在数据冗余，更新异常的问题

例如关系模型（职工号，姓名，职称，项目号，项目名称）中，**职工号->姓名，职工号->职称，而项目号->项目名称**。

显然依赖关系不满足第二范式，常用的解决办法是拆分表格，比如拆分为**职工信息表和项目信息表。**

### 第三范式条件

> 关系模型满足第二范式，**所有非主属性对任何候选关键字都不存在传递依赖**，即每个属性都跟主键有直接的关系而不是间接关系

比如Student表（学号，姓名，年龄，性别，所在院校，院校地址，院校电话）这样一个表结构，就存在上述关系。 **学号--> 所在院校 --> (院校地址，院校电话)。**我们应该拆开来，如下：

**（学号，姓名，年龄，性别，所在院校）--（所在院校，院校地址，院校电话）**

## MySQL数据库索引是如何实现的？

### 问题是什么

假设我们有这两个基本的需求：

- 根据某个值查找数据，比如 select * from user where id = 1234
- 根据区间值来查找某些数据，比如 select * from user where id > 1234 and id < 2345

### 解决问题

* **散列表**，查询性能很好，时间复杂度是O(1) ,但是散列表不支持按区间快速查找数据，所以散列表不能满足需求
* **平衡二叉查找树**：二叉查找树的性能也很高，时间复杂度是O(logn)，而且·，对树进行中序遍历，能够得到一个从小到大的序列，但是仍然不能按区间快速查找数据
* **跳表**，跳表是在链表之上加上多层索引，他支持快速插入，查找和删除数据，对应时间复杂度是O(logn)，并且，跳表也支持按区间快速查找数据，我们只需要定位到链表的节点等于起点的值，然后从这个节点开始，顺序遍历链表，直到区间对应的终点的值停下，

![img](https://img-blog.csdnimg.cn/20190530161048450.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V2ZXJ5X19kYXk=,size_16,color_FFFFFF,t_70)

实际上，数据库索引所用的数据结构和跳变非常的相似，叫做B+树，不过这是通过二叉查找树演变化过来的，而不是跳表

### 改造二叉查找树

为了让二叉查找树支持按照区间来查找数据，我们对其进行改造，树中的节点并不存储数据本身，而是只作为索引，除此之外，我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的，经过改造的二叉树就像下图这样

<img src="https://img-blog.csdnimg.cn/20190530161613826.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V2ZXJ5X19kYXk=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

改造之后，如果我们要求区间的数据，我们只需要拿区间的起始值，在树中查找，当查找到树中的某个叶子节点之后，我们在顺着链表往后遍历，直到链表中的节点数据值大于区间的终止值为止，

<img src="https://img-blog.csdnimg.cn/20190530161828723.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V2ZXJ5X19kYXk=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:33%;" />

但是，要对上千万、上亿的数据构建索引，如果将索引存储在内存中，尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。

比如，给一亿个数据构建二叉树查找树索引，那索引中会包含大约1亿个节点，每个节点假设占用16个字节，那需要大约1GB的内存。给一张表建立索引，我们需要1GB的内存空间。如果我们要给10张表建立索引，那对内存的需要是无法满足的。如何解决这个索引占用太多内存的问题呢？

借助时间换空间的思路，把索引存储在硬盘中，而非内存中。我们都知道，硬盘是一个非常慢的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。

这种方案，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。

二叉查找树，经过改造之后，支持区间查找的功能。可为了节省内存，把树存储在硬盘中，每个节点的读取（或访问），都对应一次磁盘IO操作。**树的高度就等于每次查询数据时磁盘IO操作的次数**。

如果我们**把索引构建成m叉树，树的高度就会降低**。如果对16个数据构建五叉树索引，那高度只有2，查找一个数据，对应只需要2次磁盘操作。如果 m=1000，那对一亿个数据构建索引，树的高度也只是3，最多只要3次磁盘IO就能获取到数据。磁盘IO变少了，查找数据的效率也就提高了。

<img src="https://img-blog.csdnimg.cn/20190530164525547.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V2ZXJ5X19kYXk=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:33%;" />

<img src="https://img-blog.csdnimg.cn/20190530164554371.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V2ZXJ5X19kYXk=,size_16,color_FFFFFF,t_70" alt="·" style="zoom:33%;" />

如果我们将 m 叉树实现 B+ 树索引，用代码实现出来，就是下面这个样子（假设我们给 int 类型的数据库字段添加索引，所以代码中的 keywords 是 int 类型的）：3

```java
/**
* 这是 B+ 树非叶子节点的定义。
*
* 假设 keywords=[3, 5, 8, 10]
* 4 个键值将数据分为 5 个区间：(-INF,3), [3,5), [5,8), [8,10), [10,INF)
* 5 个区间分别对应：children[0]...children[4]
*
* m 值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：
* PAGE_SIZE = (m-1)*4[keywordss 大小]+m*8[children 大小]
*/
 
class BPlusTreeNode {
public:
	static int m; // 5 叉树
	vector<int> keywords;// 键值，用来划分数据区间
	vector<BPlusTreeNode *> children;// 保存子节点指针
};
 
int BPlusTreeNode::m = 5;
 
/**
* 这是 B+ 树中叶子节点的定义。
*
* B+ 树中的叶子节点跟内部结点是不一样的,
* 叶子节点存储的是值，而非区间。
* 这个定义里，每个叶子节点存储 3 个数据行的键值及地址信息。
*
* k 值是事先计算得到的，计算的依据是让所有信息的大小正好等于页的大小：
* PAGE_SIZE = k*4[keyw.. 大小]+k*8[dataAd.. 大小]+8[prev 大小]+8[next 大小]
*/
class BPlusTreeLeafNode {
public:
	static int k;
	vector<int> keywords; // 数据的键值
	vector<long> dataAddress; // 数据地址
 
	BPlusTreeLeafNode *prev; // 这个结点在链表中的前驱结点
	BPlusTreeLeafNode *next; // 这个结点在链表中的后继结点
};
 
int BPlusTreeLeafNode::k = 3;
```

对于相同个数的数据构建 m 叉树索引，m 越大，树的高度越小，那 m 多大才最合适呢？

**不管内存中的数据，还是磁盘中的数据，操作系统都是按页（一页大小通常是4KB，**这个值可以通过 getconfig PAGE_SIZE 命令查看**）来读取的，一次会读一页的数据**。如果需要读取的数据量超过一页的大小，就会触发多次IO操作。**所以，在选择m大小时，尽量让每个节点的大小等于一页的大小，读取一个节点，只需要一次磁盘IO操作**。

<img src="https://img-blog.csdnimg.cn/20190531094111776.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2V2ZXJ5X19kYXk=,size_16,color_FFFFFF,t_70" alt="img" style="zoom:33%;" />

尽管索引可以提高数据查询的效率，但是索引会让写入数据的效率下降。因为在数据写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。

对于一个B+树来说，m 值是根据页面的大小事先计算好的，也就是说，每个节点最多只能有 m 个子节点。在数据写入的过程中，这样就可能使索引中某些节点的子节点个数超过 m，这个节点的大小超过了一个页的大小，读取一个节点，就会导致多次磁盘IO操作。

实际上，解决这个问题并不复杂。只需要将这个节点分裂成两个节点。但是节点分裂之后，其上层父节点的子节点就可能超过 m 个。不过也没关系，可以用同样的方法，将这个父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点。这个分裂过程，结合图来看，就容易理解些（图中的 B+ 树是一个三叉树。我们限定叶子节点中，数据的个数超过2个就分裂节点；非叶子节点中，子节点的个数超过3个就分裂节点）

<img src="https://static001.geekbang.org/resource/image/18/e0/1800bc80e1e05b32a042ff6873e6c2e0.jpg" alt="img" style="zoom:50%;" />

正是因为要时刻保证 B+ 树索引是一个 m 叉树，所有，索引的存在会导致数据库写入的速度降低。实际上，删除数据也会变慢。

因为在删除某个数据的时候，也要对应更新索引节点。频繁的数据删除，就会导致某些节点中，子节点的个数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。

我们可以设置一个阈值。在 B+ 树中，这个阈值等于 m/2。如果某个节点的子节点个数小于 m/2 ，我们就将它跟相邻的兄弟节点合并。不过，合并之后结点个数有可能超过m。针对这种情况，我们要以借助插入数据时的处理方法，再分裂节点。

文字描述不是很直观，我举了一个删除操作的例子，你可以对比着看下（图中的 B+ 树是一个五叉树。我们限定叶子节点中，数据的个数少于 2 个就合并节点；非叶子节点中，子节点的个数少于 3 个就合并节点）

<img src="https://static001.geekbang.org/resource/image/17/18/1730e34450dad29f062e76536622c918.jpg" alt="img" style="zoom: 50%;" />

数据库索引以及B+树的由来，就讲完了。有没有发现，B+ 树的结构和操作，跟跳表非常类似。理论上讲，对跳表稍加改造，也可以替代 B+ 树，作为数据库的索引实现。

B+树发明于1972年，跳表发明于1989年。

### 总结

数据库索引依赖底层数据结构，B+树。他通过存储在磁盘的多叉树结构，做到了时间和空间的平衡，既保证了执行的效率。又节省了内存

B+树的特点大概有以下几个

* 每个节点中子节点的个数不能超过M,也不能少于M/2
* 根节点的子节点个数可以不超过M/2这是一个例外
* M叉树只存储索引，并不储存数据，类似跳表
* 通过链表将叶子节点串联在一起，这样可以方便的按区间查找
* 一般情况，根节点会被存储在内存，其他节点存储在磁盘中

除了 B+ 树，你可能还听过 B树、B-树。实际上**B-树就是B树**，其英文翻译都是 B-Tree，这里的 “-”只是一个连接符。

而B树实际上是低版本的B+树，或者说 **B+树是B树的改进版**。**B树跟B+不同点主要集中在这几个地方：**

- B+树中的节点不存储数据，只是索引，而B树中的节点存储数据；
- B树中的叶子节点并不需要链表来串联

也就是说，B树只是一个每个节点的子节点个数不小于m/2的m叉树。

> B树

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210425160210946.png" alt="image-20210425160210946" style="zoom: 80%;" />

> B+树

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210425160328579.png" alt="image-20210425160328579" style="zoom: 80%;" />

### 扩展

> **B+树和跳表的区别**

B+树和跳表很像,都是双向链表+索引的结构,数据放在最下边,利用二分查找进行有序数列的查找

区别是什么

区别主要在索引

* 高度:同数量级的数据,跳表索引的高度会很高,IO读取次数多,影响查询性能
* 页空间浪费:MySQL默认页空间16k,跳表默认一个节点只存一个数,其他空间都会浪费

> **B+树中，将叶子节点串起来的链表，是单链表还是双向链表？为什么？**

链表是双向链表,支持前后遍历

对于B+树叶子节点,是用双向链表还是单链表,得从具体场景思考,对于数据库的数据升序和降序的要求,数据库有两种做法

* 保证查询出来的数据就是用户想要顺序
* 不保证有序性,查询出来再排序

以上两种方案,**肯定选择第一种,因为第二种浪费了时间(如果选用内存排序,还是考虑数据的量级)** 那如何能保证查询出来的数据就是有序的呢？单链表肯定做不到，只能从头往后遍历，再想想，只能选择双向链表了。此时，可能有的同学又问了：**双向链表，多出来了一倍的指针，不是会多占用空间嘛？** 答案是肯定的。可是，我们再细想下**，数据库索引本身都已经在磁盘中了，对于磁盘来说，这点空间已经微不足道了，**用这点空间换来时间肯定划算呀。顺便提一下：在实际工程应用中，双向链表应用的场景非常广泛，毕竟能大量减少链表的遍历时间

> **在哪些列上添加索引比较好**

* 比较频繁的作为查询的字段
* 唯一性太差的字段不适合添加索引
* 更新太频繁的不适合做索引
* 不会出现在where中的列不适合做索引

> 索引类型

* 主键索引：primary key，主键自动成为索引
* 唯一索引：unique 是自动成为索引的，又有索引，又有唯一性
* index：普通索引
* fulltext：全文索引，用来分词例如：select * from article where content like ‘%Java%’；但是值得注意的是，中文使用全文索引很少，外语使用全文索引比较多。中文一般使用sphinx+中文分词，只有MYISAM才支持
* 复合索引：多列合在一起的索引 create index on books（title，price）

## Hash索引和B+树索引各自的使用场景

* 如果是等值查询，那么hash索引具有绝对的优势
* 范围查询检索，就需要用到B+树
* hash索引没办法利用索引完成排序
* B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在大量重复键值的情况下，hash索引的效率是极低的，因为存在hash碰撞问题
* ==hash索引不支持多列索引的最左匹配原则==

在MySQL中，只有HEAP/MEMORY引擎表才能显式支持哈希索引（NDB也支持，但这个不常用），InnoDB引擎的自适应哈希索引（adaptive hash index）不在此列，因为这不是创建索引时可指定的。

还需要注意到：HEAP/MEMORY引擎表在mysql实例重启后，数据会丢失。

通常，B+树索引结构适用于绝大多数场景，像下面这种场景用哈希索引才更有优势：

> 在HEAP表中，如果存储的数据重复度很低（也就是说基数很大），对该列数据以等值查询为主，没有范围查询、没有排序的时候，特别适合采用哈希索引
>
> 例如这种SQL：
> SELECT … FROM t WHERE C1 = ?; — 仅等值查询

在大多数场景下，都会有范围查询、排序、分组等查询特征，用B+树索引就可以了。

##  MySQL常用的引擎？

### 什么是引擎

引擎的定义：

> 数据库引擎是用于存储处理和保护数据的核心服务，利用数据库引擎可控制访问权限和快速处理事务，从而满足企业内大多数需要处理大量数据的应用程序的要求，使用数据库引擎闯将用于联机事务处理，或联机处理数据的关系型数据库，这包括创建用于存储数据的表和用于查看，管理和保护数据安全的数据库对象（如索引，视图和存储过程）

### Innodb引擎

Innodb引擎**提供完整的ACID事务的支持,并且还提供了行级锁和外键的约束**,

他的涉及的目标就是处理大数据容量的数据库系统,他本身实际上是基于Mysql后台的完整的系统,

Mysql运行的时候,Innodb会在**内存中建立缓冲池,用于缓冲数据和索引,**但是,**MySQL5.6版本以上才支持全文索引**,同时启动比较慢,他是**不会保存表的行数的**,当进行Select count(*) from table指令的时候，需要进行扫描全表。

所以当需要使用数据库的事务时,该引擎就是首选,**由于锁的粒度小,写操作是不会锁定全表的,所以在并发度高的场景使用会提升效率**

**该存储引擎具有提交,回滚,崩溃恢复能力的事务安全,但是对比MyISAM引擎,写的效率会差一点,并且会占用更多的磁盘空间以保留数据**

### MyISAM引擎

在MySQL5.1前，他是**Mysql的默认引擎**,**但是不提供事务的支持,也不支持行级锁和外键,**

**虽然后来的版本增加了事务的支持，但是很多人眼里还是不支持事务的，并且没有崩溃恢复的功能**

因此当执行insert和update更新语句时,即执行写操作的时候需要锁定这个表,所以会导致效率贬低,

不过和Innodb不同的是,**MyISAM引擎是保存了表的行数,**于是当进行Select count(*) from table语句时，可以直接的读取已经保存的值而不需要进行扫描全表

对于**事务完整性没有要求,或者select和insert为主的应用基本上可以用这个引擎**

### MEMORY引擎

Memory引擎使用存在于内存中的内容来创建表,每个memory表只实际对应一个磁盘文件,格式是.frm,memory类型的表访问是非常的快.因为他是放在内存中的,并默认使用Hash索引,但是一旦服务关闭,表中的数据就会丢失

**memory引擎的的表可以选择BTREE索引或者HASH索引**,两者不同类型的索引有其不同的使用范围

**meomory类型的存储引擎主要用于那些内容变化不频繁的代码表,或者作为中间操作的结果表,便于高效的对中间结果计算进行分析得到最终的结果,**

> HASH索引

**优点:**

hash索引结构的特殊性,其索引效率非常高,索引的检索可以一次定位,不想B-Tree索引需要从根节点到叶子节点,这样多次的IO访问,所以Hash索引的查询效率,远高于B-Tree索引

**缺点:**

hash算法等于等值计算,所以对于范围查找就对hash索引无效,不支持,

### MERGE引擎

Merge引擎是一组MyISAM表的组合,这些表结构必须完全相同,merge表本身没有数据,对merge类型的表可以进行查询更新删除查询操作,这些操作实际上是对内部的MyISAM表进行的操作

### 引擎的选择

* 大量数据集:趋向于选择Innodb,因为它支持事务处理和故障的恢复,Innodb可以利用数据日志进行数据的恢复,主键查询Innodb也是比较快的
* 大批量的插入语句:在MyISAM引擎中执行比较的快,但是update语句在Innodb下执行的会比较快,尤其是并发量比较大的时候
* 如果只是临时存放数据,数据量不大的时候,并且不需要较高的数据安全性,可以选择将数据保存在Memory引擎,Mysql中使用该引擎作为临时表

## MyISAM和InnoDB的区别

### 锁差异

MyISAM：只支持表级锁，用户在操作数据表是，Insert，delete,update,select都会给整张表加锁，如果加锁以后能满足insert并发的情况下，可以在表的尾部插入新数据，也可以通过lock table命令来加锁，这样操作可以模拟事务，但是消耗非常大，一般只在实验的时候会用

InnoDB：InnoDB支持事务以及行级锁，是InnoDB最大的特色，当语句没有使用索引，InnoDB不能确定操作的行，这个时候就使用的意向锁，也就是表锁

### 数据库文件差异

​		**MyISAM**：

​			MyISAM属于堆表

​			在磁盘上有三个文件，每个文件以表名开头，扩展名指出文件类型

​			.frm：用于存储表的定义

​			.MYD：用于存储表的数据

​			.MYI：用于存放表索引

​			myisam表还支持三种不同的存储格式：

​			静态表(默认，但是注意数据末尾不能有空格，会被去掉)

​			动态表

​			压缩表

​		**InnoDB**：

​			InnoDB属于组织表

​			innodb有两种存储方式，共享表空间存储和多表空间存储

​			两种存储方式的表结构和myisam一样，以表名开头，扩展名是.frm

​			如果使用共享表空间，那么所有表的数据文件和索引文件都保存在一个表空间里，一个表空间可以有多个文件，通过innodb_data_file_path和innodb_data_home_dir参数设置共享表空间的位置和名字，一般共享表空间的名字叫ibdata1-n。

​			如果使用多表空间，那么每个表都有一个表空间文件用于存储每个表的数据和索引，文件名以表名开头，以.ibd为扩展名。

### 索引差异

* 关于自动增长
  * MyISAM引擎的自动增长列必须是索引，如果组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增
  * InnoDB引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列
* 关于主键
  * MyISAM允许没有主键和索引的表存在
  * MyISAM的索引都是保存行的地址
  * InnoDB如果没有设定主键或者非空的唯一索引，就会自动生成一个6个字节的主键（用户不可见）
  * InnoDB的数据是主索引的一部分，附加索引保存的是主索引的值
* 关于count()函数
  * MyISAM保有表的行数，如果使用SELECT COUNT(*) FROM TABLE ，会自动取出该值返回
  * InnoDB没有保存表的行数，如果使用SELECT COUNT(*) FROM TABLE，则会遍历整张表，消耗很大
* 全文索引
  * MyISAM支持 FULLTEXT类型的全文索引
  * InnoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。（sphinx  是一个开源软件，提供多种语言的API接口，可以优化mysql的各种查询）
* DELETE FROM TABLE
  * 使用这条命令时，InnoDB不会重新建立表，而是一条一条的删除数据，InnoDB上如果保存有大量的数据表，最好不要使用这个命令(推荐使用truncate table，不过需要用户有drop此表的权限)
* 索引保存位置
  * MyISAM的索引以表名+.MYI文件分别保存
  * InnoDB的索引和数据一起保存在表空间里

## 索引

索引定义：索引（index）是帮助MySQL高效获取数据的一种数据结构。所以索引的本质是一种数据结构。索引的目的是为了提高效率，类似于字典、目录。

可简单理解为：排好序的快速查找数据结构。一般来说，索引文件都比较大，不能存储在内存中，所以索引一般是以文件形式存储在硬盘上。

一般来说，如果没有特别指明，索引指的都是B树索引。其中聚集索引、次要索引、覆盖索引、前缀索引、唯一索引默认都是用B树。通过show index from tablename可以查看表的索引情况。

### 索引类型

1. 普通索引：没有任何限制的索引，可以在任何字段上建立，字段本身的限制条件可以判断其是否为空或唯一。
 2. 唯一索引：使用UNIQUE可以设置唯一索引，创建该索引时，索引的值必须唯一。主键是一种特殊的唯一索引
 3. 全文索引：使用FULLTEXT参数可以设置全文索引，全文索引只可以建立在CHAR、VARCHAR、TEXT字段上，查询较大的字符串类型字段时，使用该索引可以提高查询速度。该索引对大小写不敏感，在MySQL中只有MyISAM引擎支持该索引
 4. 单列索引：顾名思义，一个索引值对应一个字段，上述三种索引都可以是单列索引，只要一个索引值对应一个字段即可。
 5. 多列索引：一个索引值对应多个字段。该索引需要在创建时指定多个字段，可以通过这几个字段进行查询。在使用该索引进行查询时，索引对应的第一个字段必须要用到，否则会无效。
 6. 空间索引：使用SPATIAL参数可以设置控件索引。控件索引只能建立在控件数据类型（LINESTRING、POINT、GEOMETRY等）上，这样可以提高系统获取控件数据的效率。MySQL中只有MyISAM存储引擎支持空间索引，且该字段不能为空值。

索引结构：

1. B-TREE索引 （默认）
2. HASH索引
3. FULLTEXT索引
4. R-TREE索引

### 为什么要使用索引

1. 加快查询速度，这也是最重要的一点
2. 通过创建唯一性索引，可以保证数据库数据的唯一性

### 如何建立索引

1. 建立主键索引：ALTER TABLE `table_name` ADD PRIMARY KEY `column_name`;
2. 建立唯一索引：ALTER TABLE `table_name` ADD UNIQUE `column_name`;
3. 建立全文索引：ALTER TABLE `table_name` ADD FULLTEXT `column_name`;
4. 建立单列索引：ALTER TABLE `table_name` ADD index `index_name` `column_name`;
5. 建立多列索引：ALTER TABLE `table_name` ADD index `index_name` (`column_name1`, `column_name2`,...);
6. 查看索引：show index from `tablename`

### 何时建立索引

①建立索引的情况

1. 主键自动创建主键索引
2. 频繁作为查询的字段
3. 查询中与与其他表关联的字段，外键建立索引
4. 查询中排序的字段，如果字段排序了，用索引将会提高效率
5. 高并发下趋向使用组合索引
6. 查询中统计或分组字段
7. 尽量选择唯一性索引
8. 适合索引的列是WHERE查询子句的列，或者是连接子句中指定的列

②不建立索引的情况

  1. 表记录较少（MySQL自己可以处理，不需要索引，反而会影响效率）
  2. 经常增删改的表（在增删改的时候，索引文件同时需要更改，影响效率）
  3. 数据重复且平均分配的字段，例如国籍，性别等，不适合创建索引
  4. 频繁更新的字段
  5. WHERE里用不到的字段不适合创建索引，因为在使用WHERE时，用不到的字段会导致索引失效。

### 索引优化

索引优化的目的是为了让索引能正常使用，不失效

#### SQL关键字：Explain

explain（执行计划），使用explain可以模拟优化器执行SQL语句，从而知道MySQL是如何处理SQL语句的。通常explain主要用于分析语句或表结构性能瓶颈



通过explain+SQL语句可以得到以下几个信息

①：表的读取顺序（对应`id`）

②：数据读取操作类型（对应`select_type`）

③：SQL操作的好坏（对应`type`）

④：哪些索引可以使用（对应`possible_keys`）

⑤：哪些索引可以被使用（对应`key`）

⑥：表的直接引用（对应`ref`）

⑦：每张表有多少行被优化器查询（对应`rows`)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200302211353465.png)

1. id

   id是表的读取顺序，select查询的序列号

   id相同，读取顺序从上到下，id不同，值越大优先级越高

2. select_type

   查询的类型，主要用来区别普通查询，联合查询，子查询等复杂的查询，其值有六个

   1. SIMPER：简单查询，其中不包含子查询或union查询
   2. PRIMARY：查询中若包含复杂的子部分，最外层查询为PRIMARY，即最后加载的是PRIMARY
   3. SUBQUERY：在select或where中包含了子查询，就会被标记为SUBQUERY
   4. DERIVED：在from列表中包含的子查询会被标记为DERIVED，MySQL会递归执行这些子查询，将结果保存在临时表中。
   5. UNION：若第二个select出现在union后，则被标记为UNION，若union包含在from子句的子查询中，外层select将被标记为DERIVED。
   6. UNION RESULT

3. table

   显示table属于哪张表

4. type

   显示查询使用了哪种类型：最好到最差依次是：

   system > const > eq_ref > ref > range > index > ALL 

   * system：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现，这个也可以忽略不计
   * const ： 表示通过索引依次就找到了，const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快将如主键置于where列表中，MySQL就能将该查询转换为一个常量
   * eq_ref：唯一性索引扫描，对于每个索引建，表中只有一条记录与之匹配。常见于主键或唯一索引扫描 
   * ref：非唯一性索引扫描，返回匹配某个单独值的所有行，本质上也是一种索引访问，他返回所有匹配某个单独值的行，然而，他可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体
   * range：值检索给定范围的行，使用一个索引来选择行。key列显示使用了哪个索引。一般就是在你的where语句中出现了between and、<、>、in等的子查询。这种范围扫描索引扫描比全表扫描要好，因为他只需要开始于索引的某一点，而结束于另一点，不用扫描全部案例
   * index：full index scan ， index与all区别为index类型只遍历索引树。这通常比all快，因为索引文件通常比数据文件小（也就是说，虽然all与index都是读全表， 但index是从索引中读取的，而all是从硬盘中读的）
   * all：full table scan 将遍历全表以找到匹配的行

   一般来说，至少要达到range级别，最好能达到ref

5. possible_keys和key和key_len

   possible_keys：可能应用在表中的索引，可能是一个或多个。查询字段中包含的索引都会被列出，但是不一定使用

   key：实际中的使用，如果是NULL，则是未使用索引

   key_len：表示索引中所使用的字节数，可通过该列计算查询中使用的索引长度。在不损失精确性的情况下，`长度越短越好`。key_len显示的值为索引字段的`最大可能长度`，`并非实际使用长度`，即key_len是根据表定义计算而得，并不是通过表内检索出的

   简单来说，possible_keys是可能使用的索引，而key是实际使用的索引、

6. ref ：显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值

7. rows

   根据表统计信息及索引选用情况大致估算出找到所需记录所要读取的行数。当然该值越小越好。

8. Extra

   这一项显示十分重要的额外信息，其值有几个，优先级从高到低依次为

   Using index>Using filesort（九死一生）>Using temporary（十死无生）。

   后面两项表明SQL语句十分烂，急需优化

   * using filesort ：说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成的排序操作称为“文件排序” 
   * using temporary：使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表，常见于排序order by、分组查询group by
   * using index：表示相应的select操作中使用了覆盖索引（covering index）避免访问了表的数据行，效率不错。如果同时出现using where ，表明索引被用来执行索引键值的查找。如果没有同时出现，表明索引用来读取数据而非执行查找动作
   * using where 表明使用了where过滤
   * using join buffer：使用了连接缓存
   * impossible where ： where子句的值总是false，不能用来获取任何元组
   * select tables optimized away; 没有group by子句的情况下，基于索引优化min/max操作或者对于MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化
   * distinct

## Mysql锁分类

### 宏观

**数据库锁**

- 锁粒度小

**代码锁**

- 锁粒度大，需要封装

### 微观

#### 分类（种类）

#### 行锁 & 表锁

只有明确指定主键，才会执行行锁，否则执行表锁

- 无锁

  ```sql
  # 主键不存在
  select * from user where id = -1 for update;
  ```

- 行锁

  ```sql
  select * from user where id = 1 for update;
  select * from user where id = 1 and name = 'kkk' for update;
  ```

- 表锁

  ```sql
  select * from user where name = 'kkk' for update;
  select * from user where id <> 3 for update;
  ```



### 锁算法（机制）

#### 行锁算法

Record Lock（普通行锁）

- 键值在条件范围内
- 记录存在

Gap Lock（间隙锁）

- 对于键值不存在条件范围，叫做“间隙”（GAP），引擎就会对这个“间隙”加锁，这种机制就是Gap机制

Next-Key Lock（行 & 间隙）

- 在键值范围条件内，同时键值又不存在条件范围内

  ```sql
  # id只有1-50
  select * from user id > 49 for update
  ```

#### 表锁算法

意向锁（升级机制）

- 当一个事务带着表锁去访问了一个被加了行锁的资源，那么，此时这个行锁就会升级成意向锁，将表锁住

  ```sql
  # 事务A - 升级表锁
  select * from user where id = 10 for update;
  # 事务B - 锁表
  select * from user where name = 'kkk' for update;
  ```


自增锁

- 事务插入自增类型的列时，获取自增锁

  > 如果一个事务往表中插入自增记录，其他事物必须等待

### 实现

#### 共享锁 & 排它锁

> 行锁和表锁其实是粒度的概念，共享锁和排它锁是他们的具体实现

##### 共享锁（S)

- 允许一个事务去读一行，阻止其他事务去获取该行的排它锁
- 一般理解：能读，不能写

##### 排它锁（X）：写锁

- 允许持有排它锁的事务去读写数据，阻止其他事务获取该资源的排它锁和共享锁
- 不能获取任何锁，不代表不能读

**注意点**

- 某个事物获取事物的排它锁，其他事务不能获取该数据的任何锁，并不代表其他事务不能无锁读取该数据

  - 无锁

    ```sql
    select ... from ...
    ```

  - 共享锁

    ```sql
    select ... lock in shard mode
    ```

    > 

  - 排它锁

    ```sql
    update...
    delete...
    insert...
    select ... for update
    ```


## MySQL的行锁和表锁？

### 案例分析

>业务：因为订单重复导入，需要用脚本将订单状态为”待客服确认”且平台是”xxx”的数据批量修改为”已关闭”
>
>说明：避免直接修改订单表造成数据异常。这里用innodb_lock 表演示InnoDB的行锁。表中有三个字段：id，k(key值)，v(value值)。
>
>步骤：
>
>第一步：连接[数据库](https://www.2cto.com/database/)，这里为了方便区分命名为Transaction-A，设置autocommit为零，表示需手动提交事务。
>
>第二步：Transaction-A，执行update修改id为1的命令。
>
>第三步：新增一个连接，命名为Transaction-B，能正常修改id为2的数据。再执行修改id为1的数据命令时，却发现该命令一直处理阻塞等待中。
>
>第四步：Transaction-A，执行commit命令。Transaction-B，修改id为1的命令自动执行，等待37.51秒。

总结：**多个事务操作同一行数据时，后来的事务处于等待阻塞状态，这样可以避免了脏读等数据一致性问题，后来的事务可以操作其他行数据，解决了表锁高并发性能低的问题**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426193827013.png" alt="image-20210426193827013" style="zoom: 80%;" />

现实：当执行批量修改数据脚本的时候，行锁升级为表锁，其他操作都处于等待中

原因：InnoDB只有在通过索引条件检索数据时使用行级锁，否则使用表锁，而模拟操作正是通过id去作为检索条件，而id又是Mysql自动创建的唯一索引，所以才忽略了行锁变表锁的情况

步骤：

第一步：还原问题，Transaction-A，通过k=1更新v。Transaction-B，通过k=2更新v，命令处于阻塞等待状态。

第二步：处理问题，给需要作为查询条件的字段添加索引。用完后可以删掉。

总结：**InnoDB的行锁是针对索引加的锁，不是针对记录加的锁，并且该索引不能失效，否则都会从行锁升级为表锁**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426194739359.png" alt="image-20210426194739359" style="zoom:50%;" />

### 行锁

行锁的劣势：**开销大，加锁慢，会出现死锁**

行锁的优势：**锁的粒度小，发生锁冲突的概率低，处理并发能力强**

加锁的方式：自动加锁，对于update，delete，insert语句，InnoDB会自动给涉及数据集加排他锁，对于普通的select语句，InnoDB不会加任何锁，我们也可以显示的加锁

* 共享锁：select * from tableName where … + lock in share more
* 排他锁：select * from tableName where … + for update

**InnoDB和MySIAM最大的两点不同是**

* **InnoDB支持事务**
* **默认采用行级锁，加锁可以保证事务的一致性问题**

#### 间隙锁

> **当我们使用范围条件检索数据，请求共享或者排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁，对于键值在条件范围内但不存在的记录，叫间隙，InnoDB也会对这个间隙加锁，这种锁机制就是所谓的间隙锁（Next-Key锁)。**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426195836070.png" alt="image-20210426195836070" style="zoom: 80%;" />

**若执行条件的范围过大，则InnoDB会将整个范围内的所有索引键值全部锁定，很容易性能造成影响**

#### 排他锁

>**排他锁也称写锁，独占锁，当前写操作没有完成之前，他会阻断其他写锁和读锁**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426200241435.png" alt="image-20210426200241435" style="zoom: 67%;" />

#### 共享锁

> **共享锁，也称读锁，多用于判断数据是否存在，多个读操作可以同时进行而不会相互影响，如果事务对写锁进行修改操作，很可能会造成死锁**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426200509057.png" alt="image-20210426200509057" style="zoom: 67%;" />

#### 分析行锁定

通过检查InnoDB_row_lock 状态变量分析系统上的行锁的争夺情况 show status like ‘innodb_row_lock%’

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426200541273.png" alt="image-20210426200541273" style="zoom: 67%;" />

innodb_row_lock_current_waits: 当前正在等待锁定的数量

innodb_row_lock_time: 从系统启动到现在锁定总时间长度；非常重要的参数，

innodb_row_lock_time_avg: 每次等待所花平均时间；非常重要的参数，

innodb_row_lock_time_max: 从系统启动到现在等待最常的一次所花的时间；

innodb_row_lock_waits: 系统启动后到现在总共等待的次数；非常重要的参数。直接决定优化的方向和策略。

#### 行锁优化

* **尽可能让所有数据检索都通过索引来完成，避免无索引行或索引失效导致行锁升级为表锁**
* **尽可能避免间隙锁带来的性能下降，减少或使用何理的检索范围**
* **尽可能减少事务的粒度，比如控制事务大小，而减少锁定资源量和时间长度，从而减少锁的竞争，提升性能**
* **尽可能低级别事务隔离，隔离级别越高，并发处理能力越低**

### 表锁

表锁的优势：**开销小，加锁快，无死锁**

表锁的劣势：**锁粒度大，发生锁冲突的概率高，并发能力低**

加锁的方式：自动加锁，查询操作SELECT，会自动给涉及的所有表加**读锁**，更新操作UPDATE、DELETE、INSERT会自动给涉及的表加**写锁**，也可显式加锁

* 共享读锁：lock table tableName read;
* 独占写锁：lock table tableName write;
* 批量解锁：unlock tables;

#### 共享读锁

> **对于MyISAM表的读操作（加读锁）,不会阻塞其他进程对同一表的读操作，但会阻塞对同一表的写操作，当读锁释放后，才能执行其他进程的写操作，在锁释放前不能读取其他表**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426201942202.png" alt="image-20210426201942202" style="zoom: 67%;" />

#### 独占写锁

> **对于MyISAM表的写操作（加写锁）会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其他进程的读写操作，在锁释放前不能写其他表**

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426202152211.png" alt="image-20210426202152211" style="zoom:33%;" />

总结：**表锁，读锁会阻塞写，不会阻塞读，而写锁则会把读写都阻塞**

#### 查看加锁情况

show open tables; 1表示加锁，0表示未加锁。

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426202336938.png" alt="image-20210426202336938" style="zoom:33%;" />

#### 分析表锁定

可以通过检查table_locks_waited 和 table_locks_immediate 状态变量分析系统上的表锁定：show status like ‘table_locks%’

<img src="H:\Notes\数据库\upload\typora-user-images\image-20210426202355609.png" alt="image-20210426202355609" style="zoom:33%;" />

table_locks_immediate: 表示立即释放表锁数。

table_locks_waited: 表示需要等待的表锁数。此值越高则说明存在着越严重的表级锁争用情况。

此外，**MyISAM的读写锁调度是写优先，这也是MyISAM不适合做写为主表的存储引擎。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永久阻塞。**

#### 什么场景下用表锁

**InnoDB默认采用行锁，在未使用索引字段查询时升级为表锁**。MySQL这样设计并不是给你挖坑。它有自己的设计目的。

**即便你在条件中使用了索引字段，MySQL会根据自身的执行计划，考虑是否使用索引(所以explain命令中会有possible_key 和 key)。如果MySQL认为全表扫描效率更高，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。因此，在分析锁冲突时，别忘了检查SQL的执行计划，以确认是否真正使用了索引。**

第一种情况：**全表更新**。事务需要更新大部分或全部数据，且表又比较大。若使用行锁，会导致事务执行效率低，从而可能造成其他事务长时间锁等待和更多的锁冲突。

第二种情况：**多表查询**。事务涉及多个表，比较复杂的关联查询，很可能引起死锁，造成大量事务回滚。这种情况若能一次性锁定事务涉及的表，从而可以避免死锁、减少数据库因事务回滚带来的开销。

### 总结

* **InnoDB支持表锁和行锁，使用索引作为检索条件修改数据时采用行锁，否则采用表锁**
* **InnoDB自动给修改操作加锁，给查询操作不自动加锁**
* **行锁可能因为未使用索引而升级为表锁，所以除了检查索引是否创建的同时，也需要通过explain执行计划查询索引是否被实际使用。**
* **行锁相对于表锁来说，优势在于高并发场景下表现更为突出，毕竟锁粒度小**
* **当表的大部分数据需要修改，或者多表复杂关联查询时，使用表锁优于行锁**
* **为了保证数据一致完整性，任何一个数据库都存在锁定机制，锁定机制的优劣直接影响到一个数据库的并发处理能力和性能。**

## 乐观锁和悲观锁？

### 什么是乐观锁和悲观锁

乐观锁对应生活中乐观的人，总想着事情往好的方向发展

悲观锁对应生活中悲观的人总想着事情往坏的方向发展

这两种各有优缺点，不能说某种就一定好于另外一种

> 悲观锁

总是假设最坏的情况，每次去拿数据的时候都会认为别人会修改，所以每次拿数据都会上锁，这样被人想拿着这个数据就会阻塞直到拿到锁

**（共享资源每次只能给一个线程使用，其他线程阻塞，用完后再把资源转让给其他资源）**

传统关系型数据库里面就用到了很多这种锁的机制（**行锁，表锁，读锁，写锁）都是再做操作之前上锁**，Java中`synchronized`和`ReentrantLock`等独占锁就是悲观锁思想的实现。

> 乐观锁

假设最好的情况，每次去拿数据的时候都会认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下期间有没有人更新这个数据，

**可以使用版本号机制和CAS算法实现，**

**乐观锁适用于多读的应用类型，这样可以提高吞吐量**，像数据库提供的**write_condition机制**，其实都是提供的乐观锁

Java中`java.util.concurrent.atomic`包下面的原子变量类就是使用了乐观锁的一种实现方式**CAS**实现的。

### 乐观锁常见的实现方式

**乐观锁一般会使用版本号机制或CAS算法实现**

> 版本号机制

**一般是在数据表中加上一个版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一，当线程A要更新数据值时，在读取数据的同时也会读取version值，在更新提交的时候，若刚才读取到的version值为当前数据库version相等才更新，否则重试更新内容，直到更新成功**

举一个简单的例子：

```txt
假设数据库中帐户信息表中有一个 version 字段，当前值为 1 ；而当前帐户余额字段（ balance ）为 $100 。当需要对账户信息表进行更新的时候，需要首先读取version字段。

操作员 A 此时将其读出（ version=1 ），并从其帐户余额中扣除 $50（ $100-$50 ）。

在操作员 A 操作的过程中，操作员B 也读入此用户信息（ version=1 ），并从其帐户余额中扣除 $20 （ $100-$20 ）。

操作员 A 完成了修改工作，提交更新之前会先看数据库的版本和自己读取到的版本是否一致，一致的话，就会将数据版本号加1（ version=2 ），连同帐户扣除后余额（ balance=$50 ），提交至数据库更新，此时由于提交数据版本大于数据库记录当前版本，数据被更新，数据库记录 version 更新为 2 。

操作员 B 完成了操作，提交更新之前会先看数据库的版本和自己读取到的版本是否一致，但此时比对数据库记录版本时发现，操作员 B 提交的数据版本号为 2 ，而自己读取到的版本号为1 ，不满足 “ 当前最后更新的version与操作员第一次读取的版本号相等 “ 的乐观锁策略，因此，操作员 B 的提交被驳回。
```

这样，就避免了操作员 B 用基于 version=1 的旧数据修改的结果覆盖操作员A 的操作结果的可能。

> CAS算法

**即compare and swap（比较与交换），是一种比较有名的无锁算法，无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）**

CAS算法涉及到三个操作数

- 需要读写的内存值 V
- 进行比较的值 A
- 拟写入的新值 B

**当且仅当V的值等于A时，CAS通过原子方式用用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作），一般情况下是一个自旋操作，即不断的重试。**

### 乐观锁的缺点

ABA 问题是乐观锁一个常见的问题

> 1 ABA 问题
> 如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值，那我们就能说明它的值没有被其他线程修改过了吗？很明显是不能的，因为在这段时间它的值可能被改为其他值，然后又改回A，**那CAS操作就会误认为它从来没有被修改过。**这个问题被称为CAS操作的 "ABA"问题。

JDK 1.5 以后的 AtomicStampedReference 类就提供了此种能力，其中的 compareAndSet 方法就是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。

> 2 循环时间长开销大
> **自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销。** 如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。

> 3 只能保证一个共享变量的原子操作
> **CAS 只对单个共享变量有效，当操作涉及跨多个共享变量时 CAS 无效。**但是从 JDK 1.5开始，提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行 CAS 操作.所以我们可以使用锁或者利用AtomicReference类把多个共享变量合并成一个共享变量来操作。

### CAS与synchronized的使用情景

**简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多）**

对于资源竞争较少（线程冲突较轻）的情况，**使用synchronized同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗cpu资源；**

**而CAS基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。**

对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。

补充： Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为 “重量级锁” 。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的 偏向锁 和 轻量级锁 以及其它各种优化之后变得在某些情况下并不是那么重了。synchronized的底层实现主要依靠 Lock-Free 的队列，基本思路是 自旋后阻塞，竞争切换后继续竞争锁，稍微牺牲了公平性，但获得了高吞吐量。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。

## mysql问题排查与性能优化

> **MySQL 问题排查都有哪些手段？**

- 使用 show processlist 命令查看当前所有连接信息。
- 使用 explain 命令查询 SQL 语句执行计划。
- 开启慢查询日志，查看慢查询的 SQL。

> **如何做 MySQL 的性能优化？**

- 为搜索字段创建索引。
- 避免使用 select *，列出需要查询的字段。
- 垂直分割分表。
- 选择正确的存储引擎。

## MVCC：多版本并发控制

### 什么是MVCC

> MVCC，全称Multi-Version Concurrency Control，即多版本并发控制。
>
> MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存

**MVCC在Mysql InnoDB中的实现主要是为了提高数据库的并发性能，用更好的方式去处理读写冲突，做到即便有读写冲突时，也能做到不加锁，非阻塞并发度**

### 什么是当前读和快照读

> 当前读

像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)

这些操作都是一种当前读，为什么叫**当前读，就是读取的是版本记录的最新版本，**

**读取时还要保证其他并发事务不能修改当前记录，会对读取的记录加锁**

> 快照读

像不加锁的select操作就是快照读，**即不加锁的非阻塞读**，快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化称当前读，

之所以出现**快照读的情况，是基于提高并发性能的考虑**。快照读的实现是基于多版本并发控制，即MVCC，可以认为MVCC是行锁的一个变种

但很多情况下，避免了加锁操作，降低了开销，既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而是可能之前的历史版本

说白了，**MVCC就是为了实现读写冲突的不加锁，而这个读就是快照读，而非当前读，当前读实际上是一种加锁的操作，悲观锁的实现**

### MVCC能解决什么问题

> 数据库的并发场景

* 读读：不存在任何问题，也不需要并发控制
* 读写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读
* 写写：有线程安全问题，可能会存在更新丢失问题，

> MVCC好处

* **在并发读写数据库时，可以做到在读操作时不用阻塞读操作，提高了数据库并发读写的性能**
* **同时还可以解决脏读，幻读，不可重复读等事务隔离性问题，但不能解决更新丢失问题**

### MVCC实现原理

MVCC的目的就是多版本并发控制，在数据库中实现，就是为了解决读写冲突，他的实现原理主要时依赖记录中**的三个隐式字段，undo日志，ReadView来实现的**，

> 隐式字段

每行记录除了我们自定义的字段外，还有数据库隐式定义的

- **DB_TRX_ID**
   6byte，**最近修改(修改/插入)事务ID**：记录创建这条记录/最后一次修改该记录的事务ID
- **DB_ROLL_PTR**
   7byte，**回滚指针，**指向这条记录的上一个版本（存储于rollback segment里）
- **DB_ROW_ID**
   6byte，**隐含的自增ID（隐藏主键）**，如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引
- 实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了

<img src="https://upload-images.jianshu.io/upload_images/3133209-b45e9ebf0a3d8b14.png?imageMogr2/auto-orient/strip|imageView2/2/w/927/format/webp" alt="img" style="zoom: 67%;" />

> undo日志

* insert undo log：代表事务在insert新记录时产生的undo log，**只在事务回滚时需要，并且在事务提交后可以被立即丢弃**
* update undo log：事务在进行update，或delete时产生的undo log ,**不仅在事务回滚时需要，在快照读的时候也需要**，所以不能随便删除，**只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程同一清除**
  * 从前面的线程可以看出，为了实现InnoDB的MVCC机制，更新或者删除都只是设置一下老记录的delete_bit，并不是真正将过时的记录删除
  * 为了节省磁盘空间，InnDB有专门的purg线程来清理delete_bit为true的记录，为了不影响MVCC的正常工作，purge线程自己也维护了一个read view（这个read view相当于系统中最老活跃事务的read view）;如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。

对MVCC有帮助的实质是update undo log ，**undo log实际上就是存在rollback segment中旧记录链，**它的执行流程如下：

一、 比如一个有个事务插入persion表插入了一条新记录，记录如下，name为Jerry, age为24岁，隐式主键是1，事务ID和回滚指针，我们假设为NULL

<img src="https:////upload-images.jianshu.io/upload_images/3133209-e52ee5ae248c5a08.png?imageMogr2/auto-orient/strip|imageView2/2/w/833/format/webp" alt="img" style="zoom: 67%;" />

二、 现在来了一个事务1对该记录的name做出了修改，改为Tom

- 在事务1修改该行(记录)数据时，数据库会先对该行加排他锁

- 然后把该行数据拷贝到undo log中，作为旧记录，既在undo log中有当前行的拷贝副本

- 拷贝完毕后，修改该行name为Tom，并且修改隐藏字段的事务ID为当前事务1的ID, 我们默认从1开始，之后递增，回滚指针指向拷贝到undo log的副本记录，既表示我的上一个版本就是它

- 事务提交后，释放锁

  <img src="https:////upload-images.jianshu.io/upload_images/3133209-3b89396902dbf513.png?imageMogr2/auto-orient/strip|imageView2/2/w/843/format/webp" alt="img" style="zoom: 67%;" />

三、 又来了个事务2修改person表的同一个记录，将age修改为30岁

- 在事务2修改该行数据时，数据库也先为该行加锁

- 然后把该行数据拷贝到undo log中，作为旧记录，发现该行记录已经有undo log了，那么最新的旧数据作为链表的表头，插在该行记录的undo log最前面

- 修改该行age为30岁，并且修改隐藏字段的事务ID为当前事务2的ID, 那就是2，回滚指针指向刚刚拷贝到undo log的副本记录

- 事务提交，释放锁

  <img src="https:////upload-images.jianshu.io/upload_images/3133209-70cdae4621d5543e.png?imageMogr2/auto-orient/strip|imageView2/2/w/838/format/webp" alt="img" style="zoom: 67%;" />

从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log的链首就是最新的旧记录，链尾就是最早的旧记录（当然就像之前说的该undo log的节点可能是会purge线程清除掉，向图中的第一条insert undo log，其实在事务提交之后可能就被删除丢失了，不过这里为了演示，所以还放在这里）

#### Read View(读视图)

###### 什么是Read View?

Read View是事务进行快照读操作的时候生产的读视图(Read View)，**在该事务执行的快照读的那一刻，会生成数据库系统当前的一个快照，记录并维护系统当前活跃事务的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)**

**所以我们知道 Read View主要是用来做可见性判断的,** 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据。

Read View遵循一个可见性算法，主要是将要**被修改的数据的最新记录中的DB_TRX_ID（即当前事务ID）取出来，与系统当前其他活跃事务的ID去对比（由Read View维护），如果DB_TRX_ID跟Read View的属性做了某些比较，不符合可见性，那就通过DB_ROLL_PTR回滚指针去取出Undo Log中的DB_TRX_ID再比较，即遍历链表的DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的DB_TRX_ID, 那么这个DB_TRX_ID所在的旧记录就是当前事务能看见的最新老版本**

那么这个判断条件是什么呢？



<img src="https:////upload-images.jianshu.io/upload_images/3133209-37bbaeadbb77f36c.png?imageMogr2/auto-orient/strip|imageView2/2/w/720/format/webp" alt="img" style="zoom: 67%;" />

如上，它是一段MySQL判断可见性的一段源码，即changes_visible方法（不完全，但能看出大致逻辑），该方法展示了我们拿DB_TRX_ID去跟Read View某些属性进行怎么样的比较

在展示之前，我先简化一下Read View，我们可以把Read View简单的理解成有三个全局属性

> trx_list（名字随便取的）
>  一个数值列表，用来维护Read View生成时刻系统正活跃的事务ID
>  up_limit_id
>  记录trx_list列表中事务ID最小的ID
>  low_limit_id
>  ReadView生成时刻系统尚未分配的下一个事务ID，也就是目前已出现过的事务ID的最大值+1

- 首先比较DB_TRX_ID < up_limit_id, 如果小于，则当前事务能看到DB_TRX_ID 所在的记录，如果大于等于进入下一个判断
- 接下来判断 DB_TRX_ID 大于等于 low_limit_id , 如果大于等于则代表DB_TRX_ID 所在的记录在Read View生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断
- 判断DB_TRX_ID 是否在活跃事务之中，trx_list.contains(DB_TRX_ID)，如果在，则代表我Read View生成时刻，你这个事务还在活跃，还没有Commit，你修改的数据，我当前事务也是看不见的；如果不在，则说明，你这个事务在Read View生成之前就已经Commit了，你修改的结果，我当前事务是能看见的

#### 整体流程

我们在了解了隐式字段，undo log， 以及Read View的概念之后，就可以来看看MVCC实现的整体流程是怎么样了

整体的流程是怎么样的呢？我们可以模拟一下

- 当事务2对某行数据执行了快照读，数据库为该行数据生成一个Read View读视图，假设当前事务ID为2，此时还有事务1和事务3在活跃中，事务4在事务2快照读前一刻提交更新了，所以Read View记录了系统当前活跃事务1，3的ID，维护在一个列表上，假设我们称为trx_list

  <img src="https:////upload-images.jianshu.io/upload_images/3133209-cbf70159f8628101.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom: 50%;" />

- Read View不仅仅会通过一个列表trx_list来维护事务2执行快照读那刻系统正活跃的事务ID，还会有两个属性up_limit_id（记录trx_list列表中事务ID最小的ID），low_limit_id(记录trx_list列表中事务ID最大的ID，也有人说快照读那刻系统尚未分配的下一个事务ID也就是目前已出现过的事务ID的最大值+1，我更倾向于后者；所以在这里例子中up_limit_id就是1，low_limit_id就是4 + 1 = 5，trx_list集合的值是1,3，Read View如下图

  <img src="https:////upload-images.jianshu.io/upload_images/3133209-1d56c923cf5c6cad.png?imageMogr2/auto-orient/strip|imageView2/2/w/696/format/webp" alt="img" style="zoom: 80%;" />

- 我们的例子中，只有事务4修改过该行记录，并在事务2执行快照读前，就提交了事务，所以当前该行当前数据的undo log如下图所示；我们的事务2在快照读该行记录的时候，就会拿该行记录的DB_TRX_ID去跟up_limit_id,low_limit_id和活跃事务ID列表(trx_list)进行比较，判断当前事务2能看到该记录的版本是哪个。

  <img src="https:////upload-images.jianshu.io/upload_images/3133209-615fefab74cacee0.png?imageMogr2/auto-orient/strip|imageView2/2/w/761/format/webp" alt="img" style="zoom: 80%;" />

- 所以先拿该记录DB_TRX_ID字段记录的事务ID 4去跟Read View的的up_limit_id比较，看4是否小于up_limit_id(1)，所以不符合条件，继续判断 4 是否大于等于 low_limit_id(5)，也不符合条件，最后判断4是否处于trx_list中的活跃事务, 最后发现事务ID为4的事务不在当前活跃事务列表中, 符合可见性条件，所以事务4修改后提交的最新结果对事务2快照读时是可见的，所以事务2能读到的最新数据记录是事务4所提交的版本，而事务4提交的版本也是全局角度上最新的版本

  <img src="https:////upload-images.jianshu.io/upload_images/3133209-be5885051c52fb6a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:33%;" />

- 也正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同

<img src="https://upload-images.jianshu.io/upload_images/4301010-5b3a31b6756a4b4d.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:33%;" />

### MVCC相关问题

#### RR是如何在RC级的基础上解决不可重复读的？

当前读和快照读在RR级别下的区别：
 表1:

<img src="https:////upload-images.jianshu.io/upload_images/3133209-1d04f4bede14f4b5.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:33%;" />

表2:

<img src="https:////upload-images.jianshu.io/upload_images/3133209-ba10316e166babf6.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp" alt="img" style="zoom:33%;" />

而在表2这里的顺序中，事务B在事务A提交后的快照读和当前读都是实时的新数据400，这是为什么呢？

- 这里与上表的唯一区别仅仅是表1的事务B在事务A修改金额前快照读过一次金额数据，而表2的事务B在事务A修改金额前没有进行过快照读。

所以我们知道事务中快照读的结果是非常依赖该事务首次出现快照读的地方，即某个事务中首次出现快照读的地方非常关键，它有决定该事务后续快照读结果的能力

我们这里测试的是更新，同时删除和更新也是一样的，如果事务B的快照读是在事务A操作之后进行的，事务B的快照读也是能读取到最新的数据的

#### RC,RR级别下的InnoDB快照读有什么不同？

正是Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同

- 在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照及Read View, 将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View，所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View，所以对之后的修改不可见；
- 即RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见
- 而在RC级别下的，事务中，每次快照读都会新生成一个快照和Read View, 这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因

总之在RC隔离级别下，是每个快照读都会生成并获取最新的Read View；而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Read View, 之后的快照读获取的都是同一个Read View。

## B+树和B树的区别

### B-树概述

> B-树,这里的 B 表示 balance( 平衡的意思),B-树是一种多路自平衡的搜索树（B树是**一颗多路平衡查找树**）

类似一颗普通的平衡二叉树，不同的是B树允许每个节点拥有更多的子节点

<img src="https://upload-images.jianshu.io/upload_images/1446087-bc023e47bc74cfa1.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/800/format/webp" alt="img" style="zoom: 67%;" />

> B树的特点

* 所有键值分布在整颗树中
* 任何一个关键字出现且只出现在一个节点中
* 搜索有可能在非叶子节点结束（最好的情况O(1)就能找到数据）
* 在关键字全集做一次查找，性能逼近二分查找

### B-树深入

> 由来

定义：B-树是一类树，包括B-树，B+树，B*树等，是一棵自平衡的搜索树，他类似普通的平衡二叉树，不同的优点是B树允许每个节点有更多的子节点

B-树是专门为外部存储器设计的，如磁盘，他对于读取和写入大块数据有良好的性能，所有一般被用在文件系统和数据库中

<img src="https://upload-images.jianshu.io/upload_images/1446087-24699e1fe3fc9adf?imageMogr2/auto-orient/strip|imageView2/2/w/389/format/webp" alt="img" style="zoom:33%;" />

上图是一棵很简单的平衡二叉树，平衡二叉树是通过旋转来保持平衡的，而旋转是对整棵树的操作，若部分加载到内存中则无法完成旋转操作，其次平衡二叉树高度相对较大log n(底数为2) 这样逻辑上很接近的节点可能非常远，无法很好的利用磁盘预读（局部性原理）所以平衡二叉树在数据库文件被pass了

> 空间局性原理：如果一个存储器的某个位置被访问，那么他附近的位置也可能被访问

从磁盘角度来看B树的设计：

**索引的效率取决于磁盘的IO次数，快速索引需要有效的减少磁盘IO的次数**

如何快速索引？索引原理其实就是不断的缩小范围，平衡二叉树每次将范围分割成两个区间为了更快，B-树每次将范围分割成多个区间，区间越多，定位数据即越快越精确，如果节点为区间范围，每个节点就更大了，所以新建节点时，直接申请页大小的空间（磁盘是按照BLOCK分的，一般为512Byte，**磁盘IO一次读取若干个block，我们称之为一页**，具体大小和操作系统有关，一般为 4 k，8 k或 16 k），计算机内存分配是按页对齐的，这样就实现了一个节点只需要一次 IO。

<img src="https://upload-images.jianshu.io/upload_images/1446087-b6d4754304fa8955?imageMogr2/auto-orient/strip|imageView2/2/w/786/format/webp" alt="img" style="zoom: 50%;" />

上面是一棵简化的B-树，多叉树的好处非常明显，有效的降低了B-树的高度，为底数很大的logn,底树大小与节点的子节点的数目有关，一般一颗B-树高度在三层左右，层数低，每个节点的范围更精确，范围缩小的速度越快（比二叉树层次搜索快）

### B-树的查找

我们来看看B-树的查找，假设每个节点有 n 个 key值，被分割为 n+1 个区间，

注意，每个 key 值紧跟着 data 域，这说明B-树的 key 和 data 是聚合在一起的。一般而言，**根节点都在内存中，B-树以每个节点为一次磁盘 IO，**

比如上图中，若搜索 key 为 25 节点的 data，首先在根节点进行二分查找（因为 keys 有序，二分最快），判断 key 25 小于 key 50，所以定位到最左侧的节点，

**此时进行一次磁盘 IO，将该节点从磁盘读入内存，接着继续进行上述过程，直到找到该 key 为止。**

伪码：

```java
Data* BTreeSearch(Root *node, Key key)
{
    Data* data;

    if(root == NULL)
        return NULL;
    data = BinarySearch(node);
    if(data->key == key)
    {
        return data;
    }else{
        node = ReadDisk(data->next);
        BTreeSearch(node, key);
    }
}
```

### B+树概述

B+树是B-树的变体，也是一种多路搜索树，不同在于：

* 所有关键字都存储在叶子节点，内部节点并不存储真正的data
* 所有叶子节点增加了一个链指针

<img src="https://upload-images.jianshu.io/upload_images/1446087-30b70aaa28403803.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/800/format/webp" alt="img" style="zoom: 67%;" />

**因为内节点并不存储data，所以一般B+树的叶节点和内节点大小不同，而B-树的每个节点大小一般是相同的，为一页**

### B+树和B树的区别

* B+树内节点不存储数据，所有data存储在叶节点导致查询时间复杂度固定为logn而B-查询时间并不固定，而与key在树中的位置相关，最好为O(1).
* B+树叶节点两两相连大大增大区间访问性，可使用在范围查询等，而B-树每个节点key和data在一起，无法区间查找
  * B+树可以很好的利用局部性原理，若我们访问节点key为50，则key为50，60，62的节点将来也可能被访问，我们利用磁盘预读的原理提前将这些数据读入内存，减少了磁盘IO的次数，当然B+树也能够很好的完成范围查询
* B+树更适合外部存储，由于内节点无data域，每个节点索引的范围更大更精确
  * 由于磁盘IO数据大小是固定的，在一次IO中单个元素越小，量就越大，就意味着B+树单次磁盘IO的信息量大于B-树，从这点来看B+树相对B-树磁盘IO次数少

### 扩展知识

> 主存存取原理

目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。

<img src="https:////upload-images.jianshu.io/upload_images/1446087-0f89d70ea34a2599.png?imageMogr2/auto-orient/strip|imageView2/2/w/382/format/webp" alt="img" style="zoom:50%;" />

image

从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。

主存的存取过程如下：

当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。

写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。

这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。

> 磁盘存取原理

上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。

图6是磁盘的整体结构示意图。

<img src="https:////upload-images.jianshu.io/upload_images/1446087-07689d2c539c650b.png?imageMogr2/auto-orient/strip|imageView2/2/w/267/format/webp" alt="img" style="zoom:33%;" />

image

一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。

图7是磁盘结构的示意图。

<img src="https:////upload-images.jianshu.io/upload_images/1446087-fbd5219d2ed5d7c5.png?imageMogr2/auto-orient/strip|imageView2/2/w/269/format/webp" alt="img" style="zoom:33%;" />

image

盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，**每个段叫做一个扇区，每个扇区是磁盘的最小存储单元**。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。

当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。

> 局部性原理与磁盘预读

由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：

当一个数据被用到时，其附近的数据也通常会马上被使用。

程序运行期间所需要的数据通常比较集中。

由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。

预读的长度一般为页（page）的整倍数。**页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k）**，主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。

**所以IO一次就是读一页的大小**

## 最左匹配原则

### 什么是最左匹配原则

顾名思义：最左匹配原则，以左边的为起点任何连续的索引都能匹配上，同时遇上范围查询(>、<、between、like)就会停止匹配

b = 2 如果建立(a,b)顺序的索引，是匹配不到(a,b)索引的；可以称为不满足最左匹配原则

但是如果查询条件是a = 1 and b = 2或者a=1(又或者是b = 2 and b = 1)就可以，与条件先后无关，因为Mysql5.7开始，对索引全排列有优化，会自动优化为按组合索引的顺序进行查询

再比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，因为c字段是一个范围查询，它之后的字段会停止匹配。

```sql
select * from t where a=1 and b=1 and c =1;     #这样可以利用到定义的索引（a,b,c）

select * from t where a=1 and b=1;     #这样可以利用到定义的索引（a,b,c）

select * from t where a=1;     #这样也可以利用到定义的索引（a,b,c）

select * from t where b=1 and c=1;     #这样不可以利用到定义的索引（a,b,c）

select * from t where a=1 and c=1;     #这样可以利用到定义的索引（a,b,c），但是只能用到a

select * from t where a=1 and b>1 and c =1;     #这样a,b可以用到（a,b,c），c不可以

```

### 最左匹配原则的原理

最左匹配原则都是针对联合索引来说的，我们都知道索引底层是一棵B+树，那么联合索引当然还是一颗B+树，字只不过联合索引的键值数量不是一个，而是多个，构建一颗B+树只能根据一个值来构建，因此数据库依据联合索引最左的字段来构建B+树

假设构建一个（a,b,c）的联合索引

<img src="https://b3logfile.com/file/2020/02/1-ea5aff38.png?imageView2/2/interlace/1/format/webp" alt="1.png" style="zoom: 67%;" />

这图就是一个形如（a,b,c）联合索引的B+树，其中的非叶子节点存储的是一个非关键字的索引a,而叶子节点存储的是一个三个关键字的数据，这样可以看出，a是有序的，而b,c是无序的，但是a相同的时候，b是有序的，当b相同的时候，c是无序的

**通过对联合索引结构的了解，那么就可以很好的了解为什么最左匹配原则中如果遇到范围查询就会停下，**

以 `select * from t where a=5 and b>0 and c =1; #这样a,b可以用到（a,b,c），c不可以` 为例子，当查询到 b 的值以后（这是一个范围值），c 是无序的。所以就不能根据联合索引来确定到低该取哪一行。

### 总结

* 在InnoDB中联合索引只有确定了前一个（左侧的值）后·，才能确定下一个值，如果有范围查询的化，那么联合索引中使用范围查询的字段后的索引在该条sql中不会起作用
* 值得注意的是，in和=都可以乱序，比如有索引（a,b,c）,语句 `select * from t where c =1 and a=1 and b=1`,这样的语句也可以用到最左匹配，因为 mysql 中有一个优化器，他会分析 SQL 语句，将其优化成索引可以匹配的形式，即 `select * from t where a =1 and b=1 and c=1`



## 顺序IO和随机IO

> 顺序IO:是指读写操作的访问地址连续，在顺序IO访问，HDD所需要的磁道搜索时间显著减少，因为读写磁头可以以最小的访问移动下一个块，数据备份和日志记录是顺序IO业务

> 随机IO:是指读写时间连续，但是访问的地址不连续，随机分布在磁盘的地址空间中，产生随机IO的业务有OLTP服务，SQL，即时消息服务等

### 如何解决随机IO造成的性能损失

Mysql数据最终都会刷到磁盘上去，刷盘分随机IO和顺序IO,两者性能相差很大，大多数情况我们会改变一下设计是Mysql的随机IO变成顺序IO来改变性能

给随机IO添加缓存有更大的收益：

* 顺序IO一般只需要扫描一次数据，所以缓存对他的用处不大（收益小）
* 顺序IO比随机IO快
* 随机IO通常只要查找特定的行，但是IO的粒度是页级的，其中大部分是浪费的，而顺序IO通常发生在想要数据块上所有的行

## select....for update排他锁

### Mysql InnoDB 排他锁

例如：select * from goods where id = 1 for update;

排他锁申请的前提：没有线程对改结果集中的任何数据使用排他锁或者共享锁，否则申请会阻塞

for update 仅仅适用于InnoDB,且必须在事务块（Begin/commit）中才能生效，在事务操作时，通过 for update 语句，Mysql会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞，排他锁包括行锁，表锁。

场景分析
假设有一张商品表 goods，它包含 id，商品名称，库存量三个字段，表结构如下：

```sql
CREATE TABLE `goods` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(100) DEFAULT NULL,
  `stock` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `idx_name` (`name`) USING HASH
) ENGINE=InnoDB 

```

插入如下数据：

```mysql
INSERT INTO `goods` VALUES ('1', 'prod11', '1000');
INSERT INTO `goods` VALUES ('2', 'prod12', '1000');
INSERT INTO `goods` VALUES ('3', 'prod13', '1000');
INSERT INTO `goods` VALUES ('4', 'prod14', '1000');
INSERT INTO `goods` VALUES ('5', 'prod15', '1000');
INSERT INTO `goods` VALUES ('6', 'prod16', '1000');
INSERT INTO `goods` VALUES ('7', 'prod17', '1000');
INSERT INTO `goods` VALUES ('8', 'prod18', '1000');
INSERT INTO `goods` VALUES ('9', 'prod19', '1000');
```


一、数据一致性

假设有A、B两个用户同时各购买一件 id=1 的商品，用户A获取到的库存量为 1000，用户B获取到的库存量也为 1000，用户A完成购买后修改该商品的库存量为 999，用户B完成购买后修改该商品的库存量为 999，此时库存量数据产生了不一致。

有两种解决方案：

悲观锁方案：每次获取商品时，对该商品加排他锁。也就是在用户A获取获取 id=1 的商品信息时对该行记录加锁，期间其他用户阻塞等待访问该记录。悲观锁适合写入频繁的场景。

```sql
begin;
select * from goods where id = 1 for update;
update goods set stock = stock - 1 where id = 1;
commit;
```


乐观锁方案：每次获取商品时，不对该商品加锁。在更新数据的时候需要比较程序中的库存量与数据库中的库存量是否相等，如果相等则进行更新，反之程序重新获取库存量，再次进行比较，直到两个库存量的数值相等才进行数据更新。乐观锁适合读取频繁的场景。

```mysql
#不加锁获取 id=1 的商品对象
select * from goods where id = 1

begin;
#更新 stock 值，这里需要注意 where 条件 “stock = cur_stock”，只有程序中获取到的库存量与数据库中的库存量相等才执行更新
update goods set stock = stock - 1 where id = 1 and stock = cur_stock;
commit;
```



如果我们需要设计一个商城系统，该选择以上的哪种方案呢？

查询商品的频率比下单支付的频次高，基于以上我可能会优先考虑第二种方案（当然还有其他的方案，这里只考虑以上两种方案）。

### 行锁与表锁

1、只根据主键进行查询，并且查询到数据，主键字段产生行锁。

```mysql
begin;
select * from goods where id = 1 for update;
commit;
```

2、只根据主键进行查询，没有查询到数据，不产生锁。

```mysql
begin;
select * from goods where id = 1 for update;
commit;
```

3、根据主键、非主键含索引（name）进行查询，并且查询到数据，主键字段产生行锁，name字段产生行锁。

```
begin;
select * from goods where id = 1 and name='prod11' for update;
commit;
```

4、根据主键、非主键含索引（name）进行查询，没有查询到数据，不产生锁。

```
begin;
select * from goods where id = 1 and name='prod12' for update;
commit;
```

5、根据主键、非主键不含索引（name）进行查询，并且查询到数据，如果其他线程按主键字段进行再次查询，则主键字段产生行锁，如果其他线程按非主键不含索引字段进行查询，则非主键不含索引字段产生表锁，如果其他线程按非主键含索引字段进行查询，则非主键含索引字段产生行锁，如果索引值是枚举类型，mysql也会进行表锁

```
begin;
select * from goods where id = 1 and name='prod11' for update;
commit;
```

6、根据主键、非主键不含索引（name）进行查询，没有查询到数据，不产生锁。

```
begin;
select * from goods where id = 1 and name='prod12' for update;
commit;
```

7、根据非主键含索引（name）进行查询，并且查询到数据，name字段产生行锁。

```
begin;
select * from goods where name='prod11' for update;
commit;
```

8、根据非主键含索引（name）进行查询，没有查询到数据，不产生锁。

```
begin;
select * from goods where name='prod11' for update;
commit;
```

9、根据非主键不含索引（name）进行查询，并且查询到数据，name字段产生表锁。

```
begin;
select * from goods where name='prod11' for update;
commit;
```

10、根据非主键不含索引（name）进行查询，没有查询到数据，name字段产生表锁。

```
begin;
select * from goods where name='prod11' for update;
commit;
```

11、只根据主键进行查询，查询条件为不等于，并且查询到数据，主键字段产生表锁。

```
begin;
select * from goods where id <> 1 for update;
commit;
```


12、只根据主键进行查询，查询条件为不等于，没有查询到数据，主键字段产生表锁。

```
begin;
select * from goods where id <> 1 for update;
commit;
```

13、只根据主键进行查询，查询条件为 like，并且查询到数据，主键字段产生表锁。

```
begin;
select * from goods where id like '1' for update;
commit;
```


14、只根据主键进行查询，查询条件为 like，没有查询到数据，主键字段产生表锁。

```
begin;
select * from goods where id like '1' for update;
commit;
```

测试环境
数据库版本：5.1.48-community

数据库引擎：InnoDB Supports transactions, row-level locking, and foreign keys

数据库隔离策略：REPEATABLE-READ（系统、会话）


### 总结

* InnoDB行锁是通过索引上的索引项加锁来实现的，只要通过索引条件检索索引，InnoDB才能使用行级锁，否则，InnoDB将使用表锁
* 由于Mysql的行锁是针对索引加的锁，不是针对记录加的锁，所以虽然是访问不同行的记录，但是如果使用相同的索引键，是会出现锁冲突的
* 当表有多个索引的时候，不同的事务可以使用不同的索引锁定不同的行，另外，不论是使用主键索引，唯一索引或者普通索引，InnoDB都会使用行锁来对数据加锁
* 即使条件使用索引字段，但是否使用索引来检索数据是有Mysql通过判断不同执行计划代价来决定的，如果Mysql认为全表扫描效率更高，比如一些很小的表，他不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁，因此在分析锁冲突时，应该检查sql的执行计划，以确认是否真正使用了索引
* 检索值的数据类型与索引字段不同，虽然Mysql能够进行数据类型转换，但是却不会使用索引，从而导致InnoDB使用表锁，通过用explain检查两条Sql的执行计划

## 数据库隔离级别的实现原理

### READ_UNCOMMITED 的原理

* 事务对当前被读取的数据**不加锁**
* 事务在更新数据的瞬间（就是发生更新的瞬间），必须对其加**行级共享锁**，直到事务结束之后才能释放

表现：

* 事务1读取某行记录时，事务2也能对这行记录进行读取，更新，当事务2对改记录进行更新时，事务1在次读取该记录，能读到事务2对该记录的修改版本，即使该修改未提交
* 事务1更新某行记录时，事务2不能对这行记录更新，直到事务1提交

### READ_COMMITED 的原理:

* 事务对当前被读取的数据加**行级共享锁（当读到才加锁）**，一旦读完该行，立即释放该行级共享锁
* 事务在更新某数据的瞬间（就是发生更新的瞬间），必须对其加**行级排他锁**，直到事务结束才释放

表现：

* 事务1读取某行记录，事务2也能对这行记录进行读取，更新，当事务2对记录进行更新时，事务1在次读取该记录，读到的只能是事务2对其更新前的版本，要不就是事务2提交后的版本
* 事务1更新某行记录时，事务2对这行记录进行做更新，直到事务1结束

### REPEATABLE READ 的原理:

* 事务在读取某数据的瞬间（就是开始读取的瞬间），必须对其加**行级共享锁**，直到事务结束才释放
* 事务在更新某数据的瞬间（就是发生更新的瞬间），必须对其加**行级排他锁**，直到事务结束才能释放

表现：

* 事务1读取某行记录时，事务二也能对这行记录进行读取更新，当事务2对该记录进行更新时，事务1再次读取该记录，读到的仍然是开始读到的那个版本、
* 事务1更新某行记录时，事务2不能对这行记录做更新操作，直到事务1结束

### SERIALIZABLE 的原理:

* 事务在读取数据时，必须对其加**表级共享锁**，直到事务结束才能释放
* 事务在更新数据时，必须对其加**表级排他锁**，直到事务结束才能释放

表现：

* 事务1正在读取A表中的记录，则事务2也能读取A表，但是不能对A表子做更新，新增，删除直到事务1结束
* 事务1正在更新A表中的记录，则事务2不能读取A表中的任意记录，更不能对A表更新，删除，新增直到事务1结束

## 聚集索引，非聚集索引，覆盖索引原理

### 聚集索引

我们平时建立表的时候，都会为表加上主键，在某些关系型数据库中，如果见表不指定主键，数据库会拒绝建表的语句执行，事实上，一个加了主键的表，才能称之为表，在一个没有加主键的表，他的数据无序放在磁盘存储器上，一行一行的排序很整齐，跟我认知中的表很接近，如果给表上了主键，那么表在磁盘上的存储结构就很整齐排列的结构变为树状结构，也就是平衡树结构，换句话说，就是整个表变成了一个索引，没错，整个表变成了一个索引，也就是所谓的聚集索引

mysql的技术文档里面有如下文字：

```
If you do not define a PRIMARY KEY for your table, MySQL picks the first UNIQUE index that has only NOT NULL columns as the primary key and InnoDB uses it as the clustered index. If there is no such index in the table, InnoDB internally generates a clustered index where the rows are ordered by the row ID that InnoDB assigns to the rows in such a table. The row ID is a 6-byte field that increases monotonically as new rows are inserted. Thus, the rows ordered by the row ID are physically in insertion order. 
--------------------------------------------------------------------------------------------------------
如果你没有为你的表定义一个PRIMARY KEY，MySQL会挑选第一个只有NOT NULL列的UNIQUE索引作为主键，InnoDB会使用它作为聚类索引。 如果表中没有这样的索引，InnoDB会在内部生成一个聚类索引，其中的行是按照InnoDB分配给这种表中的行的ID排序的。 行ID是一个6字节的字段，随着新行的插入而单调地增加。因此，按行ID排序的行在物理上是按插入顺序排列的。 

```

这就是为什么一个表只能有一个主键，一个表只能有一个聚集索引，因为主键的作用就是把表的数据转换成索引（平衡树）的结构放置

<img src="https://img-blog.csdnimg.cn/20190116112317437.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0Z3VhbmdpdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom: 67%;" />

上图就是带有主键的表（聚集索引）的结构图，其中树的所有节点（底部除外）的数据都是由主键字段中的数据构成的 ，也就是我们指定的主键id字段，我们最下面的真正的表中的数据

```
select * from table where id = 1256;
```

执行上面的sql语句，首先根据id定位搭配该值所在的叶节点，然后在再通过叶节点取到id等于1256的数据行

<img src="https://img-blog.csdnimg.cn/20190116112435995.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0Z3VhbmdpdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

假如一张表有一亿条数据 ，需要查找其中某一条数据，按照常规逻辑， 一条一条的去匹配的话， 最坏的情况下需要匹配一亿次才能得到结果，用大O标记法就是O(n)最坏时间复杂度，这是无法接受的，而且这一亿条数据显然不能一次性读入内存供程序使用， 因此， 这一亿次匹配在不经缓存优化的情况下就是一亿次IO开销，以现在磁盘的IO能力和CPU的运算能力， 有可能需要几个月才能得出结果 

如果把这张表转换成平衡树结构（一棵非常茂盛和节点非常多的树），假设这棵树有10层，那么只需要10次IO开销就能查找到所需要的数据， 速度以指数级别提升，用大O标记法就是O(log n)，n是记录总树，底数是树的分叉数，结果就是树的层次数。换言之，查找次数是以树的分叉数为底，记录总数的对数，用公式来表示就是

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190116112513940.jpg)

用程序来表示就是Math.Log(100000000,10)，100000000是记录数，10是树的分叉数（真实环境下分叉数远不止10）， 结果就是查找次数，这里的结果从亿降到了个位数。因此，利用索引会使数据库查询有惊人的性能提升。

然而，事物都是有两面性的，索引能让数据库的查询速度上升，而使写入数据下降，原因很简单，因为平衡树这个结构必须一致维持在一个正确的状态，增删改查都会让平衡树的个节点中索引数据内容，破坏树结构，因此，在每次数据改变时，DBMS必须重新梳理树的结构，以保证他的正确，这会带来不小的性能开销，这就是为什么索引会给除查询以外的操作带来副作用的原因

### 非聚集索引

非聚集索引和聚集索引一样，相同是采用平衡树作为索引的数据结构，索引树结构中各节点的值来自于表中索引字段，假如给user表的name字段加上索引 ， 那么索引就是由name字段中的值构成，在数据改变时， DBMS需要一直维护索引结构的正确性。如果给表中多个字段加上索引，那么就会出现多个独立的索引结构，每个索引（非聚集索引）相互之间不存在关联

<img src="https://img-blog.csdnimg.cn/20190116112541234.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0Z3VhbmdpdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

每次给字段新建一个索引，字段的数据就会赋值出来一份，用于生成索引，因此，给表添加索引，会增加表的的体积，占用磁盘存储空间

非聚集索引和聚集索引的区别在于，通过聚集索引可以查到需要查找的数据，而通过非聚集索引可以查到记录对应的主键值，再使用主键的值通过聚集索引查找需要的数据

<img src="https://img-blog.csdnimg.cn/2019011611260686.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0Z3VhbmdpdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

不管以任何方式查询表，最终都会利用主键索引来定位到数据，聚集索引（主键）是通往真实数据所在的唯一路径

### 覆盖索引

然而，有一种例外，可以使用

```
//建立索引

create index index_birthday on user_info(birthday);

//查询生日在1991年11月1日出生用户的用户名

select user_name from user_info where birthday = '1991-11-1'

```

执行流程：

* 首先，通过非聚集索引**index_birthday查找birthday等于1991-11-1的所有记录的主键ID值**
* 然后，通过得到的主键ID执行聚集索引查找，找到主键ID值对就的真实数据（数据行）存储的位置
* 最后，从得到的真实数据中得到user_name字段的值返回，也就是取得最终的结果

我们把birthday字段上的索引改成双字段的覆盖索引

```
create index index_birthday_and_user_name on user_info(birthday, user_name);
```

这句SQL语句的执行过程就会变为:

通过非聚集索引index_birthday_and_user_name查找birthday等于1991-11-1的叶节点的内容，然而， 叶节点中除了有user_name表主键ID的值以外， user_name字段的值也在里面， 因此不需要通过主键ID值的查找数据行的真实所在， 直接取得叶节点中user_name的值返回即可。 通过这种覆盖索引直接查找的方式， 可以省略不使用覆盖索引查找的后面两个步骤， 大大的提高了查询性能，如下图

<img src="https://img-blog.csdnimg.cn/20190116112631260.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2l0Z3VhbmdpdA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

数据库索引的大致工作原理就是像文中所述， 然而细节方面可能会略有偏差，这但并不会对概念阐述的结果产生影响 

## Mysql三大日志

日志是MySQL数据库重要的组成部分，记录着数据库运行期间各种状态信息，MySQL日志主要包括错误日志，查询日志，慢查询日志，事务日志，二进制日志几大类

### binlog

**binlog用于记录数据库执行的写入性操作（不包括查询信息），以二进制的形式保存在磁盘中，binlog是MySQL逻辑日志，并且有server层进行记录，任何存储引擎MySQL数据库都会记录binlog日志**

* 逻辑日志：可以简单理解为sql语句
* 物理日志：MySQL数据最终是保存在数据页中的，物理日志就是数据页的变更

binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定的值之后，会生成新的文件来保存日志

#### 应用场景

在实际应用中，binlog的主要使用场景有两个，分别是主从复制和数据恢复

* 主从复制：在 `Master` 端开启 `binlog` ，然后将 `binlog`发送到各个 `Slave` 端， `Slave` 端重放 `binlog` 从而达到主从数据一致。
* 数据恢复：通过使用 `mysqlbinlog` 工具来恢复数据。

#### binlog的刷盘时机

对于InnoDB存储引擎而言，只有在事务提交时才会记录binlog，此时记录还在内存中，那么 `biglog`是什么时候刷到磁盘中的呢？

`mysql` 通过 `sync_binlog` 参数控制 `biglog` 的刷盘时机，取值范围是 `0-N`：

- 0：不去强制要求，由系统自行判断何时写入磁盘；
- 1：每次 `commit` 的时候都要将 `binlog` 写入磁盘；
- N：每N个事务，才会将 `binlog` 写入磁盘。

从上面可以看出， `sync_binlog` 最安全的是设置是 `1` ，这也是`MySQL 5.7.7`之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。

#### binlog日志格式

`binlog` 日志有三种格式，分别为 `STATMENT` 、 `ROW` 和 `MIXED`。

> 在 `MySQL 5.7.7` 之前，默认的格式是 `STATEMENT` ， `MySQL 5.7.7` 之后，默认值是 `ROW`。日志格式通过 `binlog-format` 指定。

- `STATMENT`：基于`SQL` 语句的复制( `statement-based replication, SBR` )，每一条会修改数据的sql语句会记录到`binlog` 中  。
  - 优点：不需要记录每一行的变化，减少了 binlog 日志量，节约了 IO  , 从而提高了性能；
  - 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、  sleep()  等 。
  
- `ROW`：基于行的复制(`row-based replication, RBR` )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。

  - 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题 ；
  - 缺点：会产生大量的日志，尤其是` alter table ` 的时候会让日志暴涨

  

- `MIXED`：基于`STATMENT` 和 `ROW` 两种模式的混合复制(`mixed-based replication, MBR` )，一般的复制使用`STATEMENT` 模式保存 `binlog` ，对于 `STATEMENT` 模式无法复制的操作使用 `ROW` 模式保存 `binlog`

### redo log

#### 为什么需要redo log

**我们都知道，事务的四大特性有一个是持久性，具体就是说，只要事务提交成功，那么对数据库做的修改就永久的保存下来了，不可能因为任何原因再回到原来的状态**

如何保证？

最简单就是在事务提交的时候，将该事务涉及的数据页全部刷到磁盘中去，但是会有严重的性能问题，主要体现在两方面

* 因为InnoDB是以页为单位进行磁盘交互的，而一个事务很可能只能修改一个数据页面里的几个字节。这个时候将完整的数据页刷到磁盘的话，就太浪费资源了
* 一个事务涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差

因此MySQL涉及了redo log 具体来说就是只记录事务对数据页进行了哪些修改，这样就完美解决了性能问题（相对而言文件更小并且是顺序IO)

#### redo log 概念

redo log 包括两个部分：一个是内存中的日志缓冲( `redo log buffer` )，另一个是磁盘上的日志文件( `redo logfile`)。

mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个记录写到redo log file。

这种先写日志，再写磁盘的技术就是MySQL经常说的WAL`(Write-Ahead Logging)`技术。

在计算机操作系统中，用户空间（user space)下的缓冲区数据一般情况下是无法写入到磁盘的，中间必须经过操作系统内核空间( `kernel space` )缓冲区( `OS Buffer` )。

因此， `redo log buffer` 写入 `redo logfile` 实际上是先写入 `OS Buffer` ，然后再通过系统调用 `fsync()` 将其刷到 `redo log file`
中，过程如下：

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9CYXE1bFlwSXc3WE9ZN0V1d24xS0hjOFBXeGhrUjdaZTk3MXpJTlhWd3FwMHNrZW9Za21PUXBHQXRjSmpjcmZYYTFkaDl2eEJERUlvbVl3OVkxdGlidGcvNjQw?x-oss-process=image/format,png" alt="img" style="zoom:50%;" />

`mysql` 支持三种将 `redo log buffer` 写入 `redo log file` 的时机，可以通过 `innodb_flush_log_at_trx_commit` 参数配置，各参数值含义如下：

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9UZVlrNDc4VzM2Qkg2cjlZaWFIbWljb0ZVZWljUm1DQzZjaGdZcFRzUlU2d1FJcDQyUjBKQ1ZLY0RwVkNsUlFFVHN6TGxpYk5CTk82blpJV1dYQTQ3NWNpYmhnLzY0MA?x-oss-process=image/format,png" alt="img" style="zoom: 80%;" />

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9CYXE1bFlwSXc3WE9ZN0V1d24xS0hjOFBXeGhrUjdaZVRpYWxCc0VtbVpaVlNpYUx5QVJBSnZYaWF6eG54UjRpY2hhbVBzVkxSNFBYNEJ5RTEwZ0RPYTVVM1EvNjQw?x-oss-process=image/format,png" alt="img" style="zoom:50%;" />

#### redo log记录形式

前面说过,redo log 实际上记录数据页的变更，而这种记录是没有必要全部保存的，因此redo log 实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9CYXE1bFlwSXc3WE9ZN0V1d24xS0hjOFBXeGhrUjdaZU5URG1zN09DUEY4VUc5ODVHaldlVnhwajFXekNYMHpleE5zanh3aWFsU0lwTDN5bVpRVUVCQWcvNjQw?x-oss-process=image/format,png" alt="img" style="zoom: 67%;" />

同时我们很容易得知，在InnoDB中，既有redo log 需要刷盘，还有数据页也需要刷盘，redo log 存在的意义就是降低对数据页刷盘的要求

在上图中， `write pos` 表示 `redo log` 当前记录的 `LSN` (逻辑序列号)位置， `check point` 表示 **数据页更改记录** 刷盘后对应 `redo log` 所处的 `LSN`(逻辑序列号)位置。

`write pos` 到 `check point` 之间的部分是 `redo log` 空着的部分，用于记录新的记录；`check point` 到 `write pos` 之间是 `redo log` 待落盘的数据页更改记录。当 `write pos`追上`check point` 时，会先推动 `check point` 向前移动，空出位置再记录新的日志。

启动 `innodb` 的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 `redo log`记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 `binlog` )要快很多。

重启`innodb` 时，首先会检查磁盘中数据页的 `LSN` ，如果数据页的`LSN` 小于日志中的 `LSN` ，则会从 `checkpoint` 开始恢复。

还有一种情况，在宕机前正处于`checkpoint` 的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 `LSN` 大于日志中的 `LSN`，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做

#### redo log与binlog区别

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9UZVlrNDc4VzM2Qkg2cjlZaWFIbWljb0ZVZWljUm1DQzZjaHhuc2pLbnBjTGgzUUw1YmVHb3ZzRzM0ZFhtOHZETmdPVkVMN0VodVcxUXk3dHBpYVFZTzVlbGcvNjQw?x-oss-process=image/format,png" alt="img" style="zoom: 80%;" />

由 `binlog` 和 `redo log` 的区别可知：`binlog` 日志只用于归档，只依靠 `binlog` 是没有 `crash-safe` 能力的。

**但只有 `redo log` 也不行，因为 `redo log` 是 `InnoDB`特有的，且日志上的记录落盘后会被覆盖掉。因此需要 `binlog`和 `redo log`二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。**

### undo log

数据库事务四大特性中有一个是原子性，具体来说就是原子性对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况

实际上，原子性底层就是通过undo log 实现的，undo log 主要记录了数据的逻辑变化，比如一条插入语句，对应一条删除语句的undo log ，对于每一个更新语句，对应一条相反的更新语句的undo log 这样发生错误时，就能回滚到事务之前的数据状态。

同时，undo log 也是MVCC(多版本并发控制) 实现的关建

## Mysql中InnoDB标为什么建议用自增列做主键

InnoDB引擎表的特点

* InnoDB引擎表是基于B+树的索引组织
  * 关于B+树
    * 所有关键字都出现在叶子节点的链表中（稠密索引）而且链表中的关建字恰好是有序的
    * 不可能在非叶子节点命中
    * 非叶子节点就相当于叶子的索引（稀疏索引）,叶子结点相当于是存储（关键字）数据的数据层
* 如果我们定义了主键，那么InnoDB会选择主键作为聚集索引，如果没有显式定义主键，那么会选择一个不包含NULL的唯一索引作为主键索引，要是也没有这样的索引，就会选择内置6个字节长的ROWID作为隐含的聚集索引（ROWID）随着行记录写入主键递增，这个ROWID像Oracle那样可以直接引用
* 数据记录本身存在于主索引的叶子节点内(大小为内存或磁盘页) 的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认15/16）则开辟一个新的页
* 如果使用的非自增主键（身份证或者学号），由于每次插入的主键的值近似于随机，因此每次新纪录都要被插入到现有索引页的中间的某个位置，MySQL不得不将记录插到合适的位置而移动数据，甚至目标页面可能被写到磁盘上从而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多的开销，同时频繁的移动，分页操作造成了很多的碎片，得到不够紧凑的索引结构

## 一条SQL语句的执行流程

### mysql架构分析

下面是mysql的一个简要架构图：

<img src="https://img.jbzj.com/file_images/article/201903/2019329112406210.png?201922911253" alt="img" style="zoom: 50%;" />

mysql主要分为Server层和存储引擎层

> **Server层**：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog日志模块。

> **存储引擎**： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持InnoDB、MyISAM、Memory等多个存储引擎，其中InnoDB引擎有自有的日志模块redolog 模块。

InnoDB 5.5.5版本作为默认引擎。

> **连接器**
>
> 主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即时管理员修改了该用户的权限，该用户也是不受影响的。

> **查询缓存**
>
> 连接建立后，执行查询语句的时候，会先查询缓存，Mysql会先校验这个sql是否执行过，以Key-Value的形式缓存在内存中，Key是查询预计，Value是结果集。如果缓存key被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。

Mysql 查询不建议使用缓存，因为对于经常更新的数据来说，缓存的有效时间太短了，往往带来的效果并不好，对于不经常更新的数据来说，使用缓存还是可以的，Mysql 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。

> **分析器**
>
> mysql 没有命中缓存，那么就会进入分析器，分析器主要是用来分析SQL语句是来干嘛的，分析器也会分为几步：
>
> 第一步，词法分析，一条SQL语句有多个字符串组成，首先要提取关键字，比如select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。
>
> 第二步，语法分析，主要就是判断你输入的sql是否正确，是否符合mysql的语法。

完成这2步之后，mysql就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。

>**优化器**
>
>优化器的作用就是它认为的最优的执行方案去执行（虽然有时候也不是最优），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。

> **执行器**
>
> 当选择了执行方案后，mysql就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。



### 语句分析

#### 查询语句

其实我们的sql可以分为2种，一种是查询，一种是更新（增加，更新，删除）。我们先分析下查询语句，语句如下：

```mysql
select * from tb_student A where A.age='18' and A.name='张三';
```

结合上面的说明，我们分析下这个语句的执行流程：

1. 先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在mysql8.0版本以前，会先查询缓存，以这条sql语句为key在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。
2. 通过分析器进行词法分析，提取sql语句的关键元素，比如提取上面这个语句是查询select，提取需要查询的表名为tb_student，需要查询所有的列，查询条件是这个表的id='1'。然后判断这个sql语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。
3. 接下来就是优化器进行确定执行方案，上面的sql语句，可以有两种执行方案：

a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是18。
b.先找出学生中年龄18岁的学生，然后再查询姓名为“张三”的学生。

那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了。

进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。

#### 更新语句

以上就是一条查询sql的执行流程，那么接下来我们看看一条更新语句如何执行的呢？sql语句如下：

```
update tb_student A set A.age='19' where A.name='张三';
```

我们来给张三修改下年龄。其实条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候要记录日志，这就会引入日志模块了，mysql 自带的日志模块式binlog（归档日志），所有的存储引擎都可以使用，我们常用的InnoDB引擎还自带了一个日志模块redo log，我们就以InnoDB模式下来探讨这个语句的执行流程。流程如下：

1. 先查询到张三这一条数据，如果有缓存，也是会用到缓存。
2. 然后拿到查询的语句，把 age 改为19，然后调用引擎API接口，写入这一行数据，InnoDB引擎把数据保存在内存中，同时记录redo log，此时redo log进入prepare状态，然后告诉执行器，执行完成了，随时可以提交。
3. 执行器收到通知后记录binlog，然后调用引擎接口，提交redo log 为提交状态。
4. 更新完成。

这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗？这就是之前mysql的模式了，MyISAM引擎是没有redo log的，那么我们知道它是不支持事务的，所以并不是说只用一个日志模块不可以，只是InnoDB引擎就是通过redo log来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么redo log 要引入prepare预提交状态？这里我们用反证法来说明下为什么要这么做？

1. 先写redo log 直接提交，然后写 binlog，假设写完redo log 后，机器挂了，binlog日志没有被写入，那么机器重启后，这台机器会通过redo log恢复数据，但是这个时候bingog并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。
2. 先写binlog，然后写redo log，假设写完了binlog，机器异常重启了，由于没有redo log，本机是无法恢复这一条记录的，但是binlog又有记录，那么和上面同样的道理，就会产生数据不一致的情况。

如果采用redo log 两阶段提交的方式就不一样了，写完binglog后，然后再提交redo log就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设redo log 处于预提交状态，binglog也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于mysql的处理机制了，mysql的处理过程如下：

1. 判断redo log 是否完整，如果判断是完整的，就立即提交。
2. 如果redo log 只是预提交但不是commit状态，这个时候就会去判断binlog是否完整，如果完整就提交 redo log, 不完整就回滚事务。

这样就解决了数据一致性的问题。

**三、总结**

1. Mysql 主要分为Server层和引擎层，Server层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用。
2. 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory等。
3. sql等执行过程分为两类，一类对于查询等过程如下：权限校验--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎
4. 对于更新等语句执行流程如下：分析器---->权限校验---->执行器--->引擎---redo log prepare--->binlog--->redo log commit